{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba89501f-3d0e-43fe-af9d-01d825d57c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ SparkContext (Very Important)\n",
    "\n",
    "## What is SparkContext?\n",
    "\n",
    "SparkContext is the entry point to Spark functionality.\n",
    "\n",
    "It represents:\n",
    "\n",
    "- Connection to cluster\n",
    "- Configuration of application\n",
    "- Resource coordination\n",
    "\n",
    "In older versions:\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp\")\n",
    "```\n",
    "\n",
    "In modern Spark:\n",
    "\n",
    "SparkSession internally creates SparkContext.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Responsibilities of SparkContext\n",
    "\n",
    "- Connects to cluster manager\n",
    "- Requests executors\n",
    "- Creates RDDs\n",
    "- Tracks application metadata\n",
    "- Distributes tasks\n",
    "- Manages broadcast variables\n",
    "- Manages accumulators\n",
    "\n",
    "---\n",
    "\n",
    "## SparkContext Architecture View\n",
    "\n",
    "```\n",
    "Application Code\n",
    "       ‚Üì\n",
    "SparkSession\n",
    "       ‚Üì\n",
    "SparkContext\n",
    "       ‚Üì\n",
    "Cluster Manager\n",
    "       ‚Üì\n",
    "Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Important SparkContext Concepts\n",
    "\n",
    "### 1Ô∏è‚É£ Broadcast Variables\n",
    "\n",
    "Used to send large read-only data to executors efficiently.\n",
    "\n",
    "```python\n",
    "broadcast_var = sc.broadcast(large_lookup_dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Accumulators\n",
    "\n",
    "Used for counters across executors.\n",
    "\n",
    "```python\n",
    "counter = sc.accumulator(0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Only One SparkContext Per JVM\n",
    "\n",
    "You cannot create multiple SparkContexts in the same application.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899b7390-a8fa-4b9a-8301-6ad71dbd0a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ SparkSession vs SparkContext ‚Äî Detailed Interview Guide\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Quick Summary\n",
    "\n",
    "| Feature | SparkContext | SparkSession |\n",
    "|----------|--------------|--------------|\n",
    "| Introduced In | Spark 1.x | Spark 2.x |\n",
    "| Purpose | Core Spark connection to cluster | Unified entry point to Spark |\n",
    "| API Type | RDD-based | DataFrame / SQL / Streaming |\n",
    "| Needed Today? | Yes (internally) | Yes (primary interface) |\n",
    "| Replaces | ‚Äî | SQLContext, HiveContext, SparkContext (partial) |\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ What is SparkContext?\n",
    "\n",
    "SparkContext is the **original entry point** to Spark (before Spark 2.0).\n",
    "\n",
    "In older versions of Spark, the main entry point was:\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Old Spark Architecture (Pre 2.0)\n",
    "\n",
    "You had to create and manage multiple contexts:\n",
    "\n",
    "- `SparkContext`\n",
    "- `SQLContext`\n",
    "- `HiveContext`\n",
    "- `StreamingContext`\n",
    "\n",
    "This made application development complex.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What SparkContext Did\n",
    "\n",
    "- Connected to the cluster\n",
    "- Resource coordination\n",
    "- Managed executors\n",
    "- RDD creation\n",
    "- Handled RDD operations\n",
    "- Provided low-level distributed processing\n",
    "- Task scheduling\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ SparkContext Responsibilities\n",
    "\n",
    "- Connects to Cluster Manager\n",
    "- Requests executors\n",
    "- Creates RDDs\n",
    "- Distributes tasks\n",
    "- Manages broadcast variables\n",
    "- Manages accumulators\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example (Old Style)\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp\")\n",
    "\n",
    "rdd = sc.textFile(\"data.txt\")\n",
    "rdd.count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Important Notes\n",
    "\n",
    "- RDD was the main abstraction\n",
    "- No structured DataFrame API (initially)\n",
    "- SQL support required separate `SQLContext`\n",
    "- Hive support required `HiveContext`\n",
    "- Streaming required `StreamingContext`\n",
    "- Multiple contexts had to be created manually\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Important Facts\n",
    "\n",
    "- Only **one SparkContext per JVM**\n",
    "- If SparkContext stops ‚Üí Application ends\n",
    "- Core object behind everything\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ What is SparkSession?\n",
    "\n",
    "SparkSession was introduced in Spark 2.0.\n",
    "\n",
    "It is a **unified entry point** for:\n",
    "\n",
    "- Spark SQL\n",
    "- DataFrame API\n",
    "- Structured Streaming\n",
    "- Hive support\n",
    "\n",
    "It internally contains:\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "\n",
    "\n",
    "When you create a SparkSession:\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "It automatically creates a SparkContext internally.\n",
    "\n",
    "You can access it like this:\n",
    "\n",
    "```python\n",
    "sc = spark.sparkContext\n",
    "print(sc.appName)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example (Modern Way)\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data.csv\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Import from `pyspark.sql`?\n",
    "\n",
    "Because `SparkSession` belongs to the Spark SQL module, which powers:\n",
    "\n",
    "- DataFrames\n",
    "- Spark SQL\n",
    "- Catalyst Optimizer\n",
    "- Tungsten Execution Engine\n",
    "\n",
    "---\n",
    "\n",
    "# üèó Why SparkSession Was Introduced\n",
    "\n",
    "To unify all contexts into a single entry point.\n",
    "\n",
    "Instead of managing:\n",
    "\n",
    "```\n",
    "SparkContext\n",
    "SQLContext\n",
    "HiveContext\n",
    "StreamingContext\n",
    "```\n",
    "\n",
    "Now we use:\n",
    "\n",
    "```\n",
    "SparkSession\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Relationship Between SparkSession and SparkContext\n",
    "\n",
    "Very important for interviews üëá\n",
    "\n",
    "```\n",
    "SparkSession\n",
    "     |\n",
    "     ‚îî‚îÄ‚îÄ SparkContext\n",
    "```\n",
    "\n",
    "SparkSession internally creates SparkContext.\n",
    "\n",
    "You can access it like this:\n",
    "\n",
    "```python\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "> SparkSession is a wrapper around SparkContext.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Internal Architecture View\n",
    "\n",
    "```\n",
    "Application Code\n",
    "       ‚Üì\n",
    "SparkSession\n",
    "       ‚Üì\n",
    "SparkContext\n",
    "       ‚Üì\n",
    "Cluster Manager\n",
    "       ‚Üì\n",
    "Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Why SparkSession Was Introduced?\n",
    "\n",
    "Before Spark 2.0, we had:\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "\n",
    "Too many contexts.\n",
    "\n",
    "SparkSession unified everything into one object.\n",
    "\n",
    "So instead of:\n",
    "\n",
    "```python\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "```\n",
    "\n",
    "Now we just use:\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ When Do You Use SparkContext Directly?\n",
    "\n",
    "Rare cases:\n",
    "\n",
    "- RDD-based operations\n",
    "- Broadcast variables\n",
    "- Accumulators\n",
    "- Low-level distributed logic\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "broadcast_var = spark.sparkContext.broadcast([1,2,3])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Interview-Ready Explanation\n",
    "\n",
    "If interviewer asks:\n",
    "\n",
    "### ‚ùì What is difference between SparkSession and SparkContext?\n",
    "\n",
    "Answer:\n",
    "\n",
    "> SparkContext is the core connection to the cluster and is used mainly for RDD operations. SparkSession is the unified entry point introduced in Spark 2.0 that wraps SparkContext and provides APIs for DataFrame, SQL, and Streaming.\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Practical Rule\n",
    "\n",
    "In modern Spark:\n",
    "\n",
    "‚úÖ Always create SparkSession  \n",
    "‚ùå Do not manually create SparkContext  \n",
    "\n",
    "SparkSession will handle it internally.\n",
    "\n",
    "---\n",
    "\n",
    "# üîü Common Interview Trap\n",
    "\n",
    "Question:\n",
    "\n",
    "Can we create multiple SparkContexts?\n",
    "\n",
    "Answer:\n",
    "\n",
    "‚ùå No. Only one SparkContext per JVM.\n",
    "\n",
    "But:\n",
    "\n",
    "You can create multiple SparkSessions using the same SparkContext.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Final Comparison\n",
    "\n",
    "| Aspect | SparkContext | SparkSession |\n",
    "|--------|--------------|--------------|\n",
    "| Level | Low-level | High-level |\n",
    "| API | RDD | DataFrame/SQL |\n",
    "| Introduced | Spark 1.x | Spark 2.x |\n",
    "| Used Today | Internally | Primary interface |\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ One-Line Memory Trick\n",
    "\n",
    "SparkContext = Engine  \n",
    "SparkSession = Dashboard + Engine\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Advanced Follow-Up (If Asked)\n",
    "\n",
    "Interviewer may ask:\n",
    "\n",
    "- What happens if SparkContext crashes?\n",
    "- Can SparkSession exist without SparkContext?\n",
    "- How does SparkSession manage Hive?\n",
    "- What is getOrCreate() doing internally?\n",
    "\n",
    "We can cover these next if you want üî•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e187ceb4-480f-4121-b513-bbb3d4c0f2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéØ Interview-Ready Answer\n",
    "\n",
    "**Why did SparkSession replace SparkContext?**\n",
    "\n",
    "SparkSession was introduced in Spark 2.0 as a unified entry point that combines SparkContext, SQLContext, HiveContext, and StreamingContext into a single object. It simplifies Spark application development and supports optimized structured APIs like DataFrames and Datasets.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö° Example: Accessing SparkContext from SparkSession\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Demo App\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(sc)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Key Takeaways\n",
    "\n",
    "- SparkContext is low-level and RDD-based.\n",
    "- SparkSession is high-level and structured API based.\n",
    "- SparkSession internally manages SparkContext.\n",
    "- Modern Spark development uses SparkSession.\n",
    "- Always use SparkSession in production applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00a6b03-dcee-4359-86b7-d8eeb1c673a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Default Spark Session from Databricks\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b907156c-165d-4b8f-b132-5623ac6268b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Automatically gets created by Databricks (SparkSession)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52449582-b58c-41b4-bb50-ca461768b899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# How to create SparkSession and Spark Context manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62226dc-f635-4e1e-99a1-0759d96efb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "# from pyspark.sql import SparkSession# spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Fundamentals\").getOrCreate()\n",
    "\n",
    "\n",
    "spark2 = SparkSession.builder.appName(\"Spark Fundamentals\")\\\n",
    "  .config(\"spark.sql.warehouse.dir\", \"file:///databricks/driver/spark-warehouse\")\\\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"4\")\\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad39b02-89ec-4994-b01c-91f6d451e046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68066f6-122d-4364-9bb8-598c57c7a677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Understanding SparkSession `.config()` in Spark\n",
    "\n",
    "---\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "spark2 = SparkSession.builder \\\n",
    "  .appName(\"Spark Fundamentals\") \\\n",
    "  .config(\"spark.sql.warehouse.dir\", \"file:///databricks/driver/spark-warehouse\") \\\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "  .getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is `.config()`?\n",
    "\n",
    "`.config()` is used to set **Spark configuration properties** while creating a SparkSession.\n",
    "\n",
    "It allows you to:\n",
    "\n",
    "- Override default Spark settings\n",
    "- Tune performance\n",
    "- Configure storage paths\n",
    "- Set execution parameters\n",
    "\n",
    "Internally, these configurations are passed to:\n",
    "\n",
    "```\n",
    "SparkConf ‚Üí SparkContext ‚Üí Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ What is `spark.sql.warehouse.dir`?\n",
    "\n",
    "```\n",
    ".config(\"spark.sql.warehouse.dir\", \"file:///databricks/driver/spark-warehouse\")\n",
    "```\n",
    "\n",
    "## üîπ What It Does\n",
    "\n",
    "This sets the **default warehouse directory** where Spark stores:\n",
    "\n",
    "- Managed tables\n",
    "- Hive tables\n",
    "- Metadata files\n",
    "- Default database storage\n",
    "\n",
    "When you run:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE test_table (...)\n",
    "```\n",
    "\n",
    "Spark stores the table data in the warehouse directory.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ From Where Is This Path Taken?\n",
    "\n",
    "In Databricks:\n",
    "\n",
    "```\n",
    "file:///databricks/driver/spark-warehouse\n",
    "```\n",
    "\n",
    "### Breakdown:\n",
    "\n",
    "- `file://` ‚Üí Local file system\n",
    "- `/databricks/driver/` ‚Üí Driver node's local disk\n",
    "- `spark-warehouse` ‚Üí Default warehouse folder\n",
    "\n",
    "### Important:\n",
    "\n",
    "This is **Databricks-specific local driver storage path**.\n",
    "\n",
    "If running locally (non-Databricks), default is usually:\n",
    "\n",
    "```\n",
    "file:///user/hive/warehouse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ What is `spark.sql.shuffle.partitions`?\n",
    "\n",
    "```\n",
    ".config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "```\n",
    "\n",
    "## üîπ What It Controls\n",
    "\n",
    "This defines the **number of partitions created during shuffle operations**.\n",
    "\n",
    "Shuffle happens during:\n",
    "\n",
    "- groupBy()\n",
    "- join()\n",
    "- distinct()\n",
    "- orderBy()\n",
    "- repartition()\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Default Value\n",
    "\n",
    "Default = **200 partitions**\n",
    "\n",
    "That means:\n",
    "\n",
    "When you do:\n",
    "\n",
    "```python\n",
    "df.groupBy(\"id\").count()\n",
    "```\n",
    "\n",
    "Spark will create 200 shuffle partitions by default.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Set It to 4?\n",
    "\n",
    "In small datasets (like learning or development):\n",
    "\n",
    "- 200 partitions is too many\n",
    "- Creates unnecessary small tasks\n",
    "- Slows down performance\n",
    "\n",
    "So we reduce it to:\n",
    "\n",
    "```\n",
    "4 partitions\n",
    "```\n",
    "\n",
    "This makes Spark create only 4 shuffle tasks.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Important Interview Concept\n",
    "\n",
    "### What is Shuffle?\n",
    "\n",
    "Shuffle is the process of:\n",
    "\n",
    "- Redistributing data across executors\n",
    "- Based on key (for join/groupBy)\n",
    "\n",
    "It is:\n",
    "\n",
    "- Expensive\n",
    "- Disk + network heavy\n",
    "- Performance critical\n",
    "\n",
    "That‚Äôs why tuning `spark.sql.shuffle.partitions` is important.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ How to Check These Configs?\n",
    "\n",
    "After creating SparkSession:\n",
    "\n",
    "```python\n",
    "spark.conf.get(\"spark.sql.warehouse.dir\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "```\n",
    "\n",
    "Or list all configs:\n",
    "\n",
    "```python\n",
    "spark.sparkContext.getConf().getAll()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Summary\n",
    "\n",
    "| Config | Purpose |\n",
    "|--------|----------|\n",
    "| spark.sql.warehouse.dir | Location to store managed tables |\n",
    "| spark.sql.shuffle.partitions | Number of partitions during shuffle |\n",
    "| .config() | Used to set Spark runtime properties |\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Real Production Note\n",
    "\n",
    "In production environments:\n",
    "\n",
    "Warehouse path usually points to:\n",
    "\n",
    "- HDFS\n",
    "- S3\n",
    "- ADLS\n",
    "- DBFS (Databricks File System)\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "dbfs:/user/hive/warehouse\n",
    "s3://bucket/warehouse/\n",
    "abfss://container@storage.dfs.core.windows.net/warehouse/\n",
    "```\n",
    "\n",
    "Not local driver storage.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Final Understanding\n",
    "\n",
    "`.config()` customizes Spark behavior at runtime.\n",
    "\n",
    "- Warehouse config ‚Üí Storage location\n",
    "- Shuffle config ‚Üí Performance tuning\n",
    "\n",
    "Both are very important for Data Engineering interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaf6053-50c5-42f8-a008-19e223527b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f45cb-1425-4c69-9c33-eb60f1245c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Spark Session & Spark Context 2026-02-17",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
