{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba89501f-3d0e-43fe-af9d-01d825d57c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ SparkContext (Very Important)\n",
    "\n",
    "## What is SparkContext?\n",
    "\n",
    "SparkContext is the entry point to Spark functionality.\n",
    "\n",
    "It represents:\n",
    "\n",
    "- Connection to cluster\n",
    "- Configuration of application\n",
    "- Resource coordination\n",
    "\n",
    "In older versions:\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp\")\n",
    "```\n",
    "\n",
    "In modern Spark:\n",
    "\n",
    "SparkSession internally creates SparkContext.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Responsibilities of SparkContext\n",
    "\n",
    "- Connects to cluster manager\n",
    "- Requests executors\n",
    "- Creates RDDs\n",
    "- Tracks application metadata\n",
    "- Distributes tasks\n",
    "- Manages broadcast variables\n",
    "- Manages accumulators\n",
    "\n",
    "---\n",
    "\n",
    "## SparkContext Architecture View\n",
    "\n",
    "```\n",
    "Application Code\n",
    "       ‚Üì\n",
    "SparkSession\n",
    "       ‚Üì\n",
    "SparkContext\n",
    "       ‚Üì\n",
    "Cluster Manager\n",
    "       ‚Üì\n",
    "Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Important SparkContext Concepts\n",
    "\n",
    "### 1Ô∏è‚É£ Broadcast Variables\n",
    "\n",
    "Used to send large read-only data to executors efficiently.\n",
    "\n",
    "```python\n",
    "broadcast_var = sc.broadcast(large_lookup_dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Accumulators\n",
    "\n",
    "Used for counters across executors.\n",
    "\n",
    "```python\n",
    "counter = sc.accumulator(0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Only One SparkContext Per JVM\n",
    "\n",
    "You cannot create multiple SparkContexts in the same application.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "899b7390-a8fa-4b9a-8301-6ad71dbd0a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ SparkSession vs SparkContext ‚Äî Detailed Interview Guide\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Quick Summary\n",
    "\n",
    "| Feature | SparkContext | SparkSession |\n",
    "|----------|--------------|--------------|\n",
    "| Introduced In | Spark 1.x | Spark 2.x |\n",
    "| Purpose | Core Spark connection to cluster | Unified entry point to Spark |\n",
    "| API Type | RDD-based | DataFrame / SQL / Streaming |\n",
    "| Needed Today? | Yes (internally) | Yes (primary interface) |\n",
    "| Replaces | ‚Äî | SQLContext, HiveContext, SparkContext (partial) |\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ What is SparkContext?\n",
    "\n",
    "SparkContext is the **original entry point** to Spark (before Spark 2.0).\n",
    "\n",
    "In older versions of Spark, the main entry point was:\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(\"local\", \"My App\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß± Old Spark Architecture (Pre 2.0)\n",
    "\n",
    "You had to create and manage multiple contexts:\n",
    "\n",
    "- `SparkContext`\n",
    "- `SQLContext`\n",
    "- `HiveContext`\n",
    "- `StreamingContext`\n",
    "\n",
    "This made application development complex.\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ What SparkContext Did\n",
    "\n",
    "- Connected to the cluster\n",
    "- Resource coordination\n",
    "- Managed executors\n",
    "- RDD creation\n",
    "- Handled RDD operations\n",
    "- Provided low-level distributed processing\n",
    "- Task scheduling\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ SparkContext Responsibilities\n",
    "\n",
    "- Connects to Cluster Manager\n",
    "- Requests executors\n",
    "- Creates RDDs\n",
    "- Distributes tasks\n",
    "- Manages broadcast variables\n",
    "- Manages accumulators\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example (Old Style)\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp\")\n",
    "\n",
    "rdd = sc.textFile(\"data.txt\")\n",
    "rdd.count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Important Notes\n",
    "\n",
    "- RDD was the main abstraction\n",
    "- No structured DataFrame API (initially)\n",
    "- SQL support required separate `SQLContext`\n",
    "- Hive support required `HiveContext`\n",
    "- Streaming required `StreamingContext`\n",
    "- Multiple contexts had to be created manually\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Important Facts\n",
    "\n",
    "- Only **one SparkContext per JVM**\n",
    "- If SparkContext stops ‚Üí Application ends\n",
    "- Core object behind everything\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ What is SparkSession?\n",
    "\n",
    "SparkSession was introduced in Spark 2.0.\n",
    "\n",
    "It is a **unified entry point** for:\n",
    "\n",
    "- Spark SQL\n",
    "- DataFrame API\n",
    "- Structured Streaming\n",
    "- Hive support\n",
    "\n",
    "It internally contains:\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "\n",
    "\n",
    "When you create a SparkSession:\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "It automatically creates a SparkContext internally.\n",
    "\n",
    "You can access it like this:\n",
    "\n",
    "```python\n",
    "sc = spark.sparkContext\n",
    "print(sc.appName)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example (Modern Way)\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data.csv\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Import from `pyspark.sql`?\n",
    "\n",
    "Because `SparkSession` belongs to the Spark SQL module, which powers:\n",
    "\n",
    "- DataFrames\n",
    "- Spark SQL\n",
    "- Catalyst Optimizer\n",
    "- Tungsten Execution Engine\n",
    "\n",
    "---\n",
    "\n",
    "# üèó Why SparkSession Was Introduced\n",
    "\n",
    "To unify all contexts into a single entry point.\n",
    "\n",
    "Instead of managing:\n",
    "\n",
    "```\n",
    "SparkContext\n",
    "SQLContext\n",
    "HiveContext\n",
    "StreamingContext\n",
    "```\n",
    "\n",
    "Now we use:\n",
    "\n",
    "```\n",
    "SparkSession\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Relationship Between SparkSession and SparkContext\n",
    "\n",
    "Very important for interviews üëá\n",
    "\n",
    "```\n",
    "SparkSession\n",
    "     |\n",
    "     ‚îî‚îÄ‚îÄ SparkContext\n",
    "```\n",
    "\n",
    "SparkSession internally creates SparkContext.\n",
    "\n",
    "You can access it like this:\n",
    "\n",
    "```python\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "> SparkSession is a wrapper around SparkContext.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Internal Architecture View\n",
    "\n",
    "```\n",
    "Application Code\n",
    "       ‚Üì\n",
    "SparkSession\n",
    "       ‚Üì\n",
    "SparkContext\n",
    "       ‚Üì\n",
    "Cluster Manager\n",
    "       ‚Üì\n",
    "Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Why SparkSession Was Introduced?\n",
    "\n",
    "Before Spark 2.0, we had:\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "\n",
    "Too many contexts.\n",
    "\n",
    "SparkSession unified everything into one object.\n",
    "\n",
    "So instead of:\n",
    "\n",
    "```python\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "```\n",
    "\n",
    "Now we just use:\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ When Do You Use SparkContext Directly?\n",
    "\n",
    "Rare cases:\n",
    "\n",
    "- RDD-based operations\n",
    "- Broadcast variables\n",
    "- Accumulators\n",
    "- Low-level distributed logic\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "broadcast_var = spark.sparkContext.broadcast([1,2,3])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Interview-Ready Explanation\n",
    "\n",
    "If interviewer asks:\n",
    "\n",
    "### ‚ùì What is difference between SparkSession and SparkContext?\n",
    "\n",
    "Answer:\n",
    "\n",
    "> SparkContext is the core connection to the cluster and is used mainly for RDD operations. SparkSession is the unified entry point introduced in Spark 2.0 that wraps SparkContext and provides APIs for DataFrame, SQL, and Streaming.\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Practical Rule\n",
    "\n",
    "In modern Spark:\n",
    "\n",
    "‚úÖ Always create SparkSession  \n",
    "‚ùå Do not manually create SparkContext  \n",
    "\n",
    "SparkSession will handle it internally.\n",
    "\n",
    "---\n",
    "\n",
    "# üîü Common Interview Trap\n",
    "\n",
    "Question:\n",
    "\n",
    "Can we create multiple SparkContexts?\n",
    "\n",
    "Answer:\n",
    "\n",
    "‚ùå No. Only one SparkContext per JVM.\n",
    "\n",
    "But:\n",
    "\n",
    "You can create multiple SparkSessions using the same SparkContext.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Final Comparison\n",
    "\n",
    "| Aspect | SparkContext | SparkSession |\n",
    "|--------|--------------|--------------|\n",
    "| Level | Low-level | High-level |\n",
    "| API | RDD | DataFrame/SQL |\n",
    "| Introduced | Spark 1.x | Spark 2.x |\n",
    "| Used Today | Internally | Primary interface |\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ One-Line Memory Trick\n",
    "\n",
    "SparkContext = Engine  \n",
    "SparkSession = Dashboard + Engine\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Advanced Follow-Up (If Asked)\n",
    "\n",
    "Interviewer may ask:\n",
    "\n",
    "- What happens if SparkContext crashes?\n",
    "- Can SparkSession exist without SparkContext?\n",
    "- How does SparkSession manage Hive?\n",
    "- What is getOrCreate() doing internally?\n",
    "\n",
    "We can cover these next if you want üî•\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e187ceb4-480f-4121-b513-bbb3d4c0f2f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üéØ Interview-Ready Answer\n",
    "\n",
    "**Why did SparkSession replace SparkContext?**\n",
    "\n",
    "SparkSession was introduced in Spark 2.0 as a unified entry point that combines SparkContext, SQLContext, HiveContext, and StreamingContext into a single object. It simplifies Spark application development and supports optimized structured APIs like DataFrames and Datasets.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚ö° Example: Accessing SparkContext from SparkSession\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Demo App\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Access SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(sc)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Key Takeaways\n",
    "\n",
    "- SparkContext is low-level and RDD-based.\n",
    "- SparkSession is high-level and structured API based.\n",
    "- SparkSession internally manages SparkContext.\n",
    "- Modern Spark development uses SparkSession.\n",
    "- Always use SparkSession in production applications."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c00a6b03-dcee-4359-86b7-d8eeb1c673a8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Default Spark Session from Databricks\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b907156c-165d-4b8f-b132-5623ac6268b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Automatically gets created by Databricks (SparkSession)\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52449582-b58c-41b4-bb50-ca461768b899",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# How to create SparkSession and Spark Context manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62226dc-f635-4e1e-99a1-0759d96efb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "# from pyspark.sql import SparkSession# spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Fundamentals\").getOrCreate()\n",
    "\n",
    "\n",
    "spark2 = SparkSession.builder.appName(\"Spark Fundamentals\")\\\n",
    "  .config(\"spark.sql.warehouse.dir\", \"file:///databricks/driver/spark-warehouse\")\\\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"4\")\\\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad39b02-89ec-4994-b01c-91f6d451e046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e68066f6-122d-4364-9bb8-598c57c7a677",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Understanding SparkSession `.config()` in Spark\n",
    "\n",
    "---\n",
    "\n",
    "## Code Example\n",
    "\n",
    "```python\n",
    "spark2 = SparkSession.builder \\\n",
    "  .appName(\"Spark Fundamentals\") \\\n",
    "  .config(\"spark.sql.warehouse.dir\", \"file:///databricks/driver/spark-warehouse\") \\\n",
    "  .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "  .getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is `.config()`?\n",
    "\n",
    "`.config()` is used to set **Spark configuration properties** while creating a SparkSession.\n",
    "\n",
    "It allows you to:\n",
    "\n",
    "- Override default Spark settings\n",
    "- Tune performance\n",
    "- Configure storage paths\n",
    "- Set execution parameters\n",
    "\n",
    "Internally, these configurations are passed to:\n",
    "\n",
    "```\n",
    "SparkConf ‚Üí SparkContext ‚Üí Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ What is `spark.sql.warehouse.dir`?\n",
    "\n",
    "```\n",
    ".config(\"spark.sql.warehouse.dir\", \"file:///databricks/driver/spark-warehouse\")\n",
    "```\n",
    "\n",
    "## üîπ What It Does\n",
    "\n",
    "This sets the **default warehouse directory** where Spark stores:\n",
    "\n",
    "- Managed tables\n",
    "- Hive tables\n",
    "- Metadata files\n",
    "- Default database storage\n",
    "\n",
    "When you run:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE test_table (...)\n",
    "```\n",
    "\n",
    "Spark stores the table data in the warehouse directory.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ From Where Is This Path Taken?\n",
    "\n",
    "In Databricks:\n",
    "\n",
    "```\n",
    "file:///databricks/driver/spark-warehouse\n",
    "```\n",
    "\n",
    "### Breakdown:\n",
    "\n",
    "- `file://` ‚Üí Local file system\n",
    "- `/databricks/driver/` ‚Üí Driver node's local disk\n",
    "- `spark-warehouse` ‚Üí Default warehouse folder\n",
    "\n",
    "### Important:\n",
    "\n",
    "This is **Databricks-specific local driver storage path**.\n",
    "\n",
    "If running locally (non-Databricks), default is usually:\n",
    "\n",
    "```\n",
    "file:///user/hive/warehouse\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ What is `spark.sql.shuffle.partitions`?\n",
    "\n",
    "```\n",
    ".config(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "```\n",
    "\n",
    "## üîπ What It Controls\n",
    "\n",
    "This defines the **number of partitions created during shuffle operations**.\n",
    "\n",
    "Shuffle happens during:\n",
    "\n",
    "- groupBy()\n",
    "- join()\n",
    "- distinct()\n",
    "- orderBy()\n",
    "- repartition()\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Default Value\n",
    "\n",
    "Default = **200 partitions**\n",
    "\n",
    "That means:\n",
    "\n",
    "When you do:\n",
    "\n",
    "```python\n",
    "df.groupBy(\"id\").count()\n",
    "```\n",
    "\n",
    "Spark will create 200 shuffle partitions by default.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Set It to 4?\n",
    "\n",
    "In small datasets (like learning or development):\n",
    "\n",
    "- 200 partitions is too many\n",
    "- Creates unnecessary small tasks\n",
    "- Slows down performance\n",
    "\n",
    "So we reduce it to:\n",
    "\n",
    "```\n",
    "4 partitions\n",
    "```\n",
    "\n",
    "This makes Spark create only 4 shuffle tasks.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Important Interview Concept\n",
    "\n",
    "### What is Shuffle?\n",
    "\n",
    "Shuffle is the process of:\n",
    "\n",
    "- Redistributing data across executors\n",
    "- Based on key (for join/groupBy)\n",
    "\n",
    "It is:\n",
    "\n",
    "- Expensive\n",
    "- Disk + network heavy\n",
    "- Performance critical\n",
    "\n",
    "That‚Äôs why tuning `spark.sql.shuffle.partitions` is important.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ How to Check These Configs?\n",
    "\n",
    "After creating SparkSession:\n",
    "\n",
    "```python\n",
    "spark.conf.get(\"spark.sql.warehouse.dir\")\n",
    "spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "```\n",
    "\n",
    "Or list all configs:\n",
    "\n",
    "```python\n",
    "spark.sparkContext.getConf().getAll()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Summary\n",
    "\n",
    "| Config | Purpose |\n",
    "|--------|----------|\n",
    "| spark.sql.warehouse.dir | Location to store managed tables |\n",
    "| spark.sql.shuffle.partitions | Number of partitions during shuffle |\n",
    "| .config() | Used to set Spark runtime properties |\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Real Production Note\n",
    "\n",
    "In production environments:\n",
    "\n",
    "Warehouse path usually points to:\n",
    "\n",
    "- HDFS\n",
    "- S3\n",
    "- ADLS\n",
    "- DBFS (Databricks File System)\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "dbfs:/user/hive/warehouse\n",
    "s3://bucket/warehouse/\n",
    "abfss://container@storage.dfs.core.windows.net/warehouse/\n",
    "```\n",
    "\n",
    "Not local driver storage.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Final Understanding\n",
    "\n",
    "`.config()` customizes Spark behavior at runtime.\n",
    "\n",
    "- Warehouse config ‚Üí Storage location\n",
    "- Shuffle config ‚Üí Performance tuning\n",
    "\n",
    "Both are very important for Data Engineering interviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaf6053-50c5-42f8-a008-19e223527b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5503c801-567e-4802-9f72-63096bea6428",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Creating and Displaying RDD in Apache Spark (PySpark)\n",
    "\n",
    "## üîπ What is an RDD?\n",
    "\n",
    "RDD (Resilient Distributed Dataset) is the fundamental distributed data structure in Spark.\n",
    "\n",
    "It is:\n",
    "- Distributed across cluster\n",
    "- Fault tolerant\n",
    "- Immutable\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 1Ô∏è‚É£ Create RDD from a List\n",
    "\n",
    "```python\n",
    "data = [10, 20, 30, 40, 50]\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data)\n",
    "\n",
    "rdd\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 2Ô∏è‚É£ Display RDD Data\n",
    "\n",
    "RDD does not have `.show()` like DataFrame.\n",
    "\n",
    "### ‚úÖ Use `collect()`\n",
    "\n",
    "```python\n",
    "rdd.collect()\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "[10, 20, 30, 40, 50]\n",
    "```\n",
    "\n",
    "‚ö†Ô∏è Collect brings all data to driver (not recommended for large datasets).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Use `take(n)`\n",
    "\n",
    "```python\n",
    "rdd.take(3)\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "[10, 20, 30]\n",
    "```\n",
    "\n",
    "Better for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Use `foreach()` (Print each element)\n",
    "\n",
    "```python\n",
    "rdd.foreach(print)\n",
    "```\n",
    "\n",
    "Note: In cluster mode, output appears in executor logs.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 3Ô∏è‚É£ Create RDD from a Text File\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.textFile(\"path/to/file.txt\")\n",
    "\n",
    "rdd.take(5)\n",
    "```\n",
    "\n",
    "Each line becomes one RDD element.\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 4Ô∏è‚É£ Create RDD from Existing DataFrame\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"path/to/file.csv\", header=True)\n",
    "\n",
    "rdd = df.rdd\n",
    "\n",
    "rdd.take(5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 5Ô∏è‚É£ Check Number of Partitions\n",
    "\n",
    "```python\n",
    "rdd.getNumPartitions()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ 6Ô∏è‚É£ Basic Transformation Example\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "\n",
    "squared = rdd.map(lambda x: x * x)\n",
    "\n",
    "squared.collect()\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "[1, 4, 9, 16, 25]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üîπ Important\n",
    "\n",
    "RDD is low-level API.\n",
    "\n",
    "In modern Spark:\n",
    "- Prefer DataFrame / Dataset API\n",
    "- RDD is mainly used for:\n",
    "  - Complex transformations\n",
    "  - Custom distributed logic\n",
    "  - Legacy code\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Final Example\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([\"Apple\", \"Banana\", \"Orange\"])\n",
    "\n",
    "print(rdd.collect())\n",
    "```\n",
    "\n",
    "Output:\n",
    "```\n",
    "['Apple', 'Banana', 'Orange']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# üî• **RDD Core Properties ‚Äì Explained with Examples**\n",
    "\n",
    "RDD = Resilient Distributed Dataset\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Immutable\n",
    "\n",
    "## üîπ What Does Immutable Mean?\n",
    "\n",
    "Once an RDD is created, it **cannot be modified**.\n",
    "\n",
    "Every transformation creates a **new RDD**.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example\n",
    "\n",
    "```python\n",
    "rdd1 = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "\n",
    "print(rdd1.collect())  # [1, 2, 3, 4]\n",
    "print(rdd2.collect())  # [2, 4, 6, 8]\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- `rdd1` remains unchanged.\n",
    "- `map()` created a new RDD (`rdd2`).\n",
    "- Original RDD is never modified.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Distributed\n",
    "\n",
    "## üîπ What Does Distributed Mean?\n",
    "\n",
    "Data is split into **multiple partitions**  \n",
    "Each partition is processed in parallel on different executors.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8], 4)\n",
    "\n",
    "print(rdd.getNumPartitions())\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "4\n",
    "```\n",
    "\n",
    "### What This Means:\n",
    "\n",
    "- Data is divided into 4 partitions.\n",
    "- Each partition can run on different executors.\n",
    "- Each partition ‚Üí 1 task.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Visual Representation\n",
    "\n",
    "```\n",
    "RDD\n",
    " ‚îú‚îÄ‚îÄ Partition 1 ‚Üí Executor 1\n",
    " ‚îú‚îÄ‚îÄ Partition 2 ‚Üí Executor 2\n",
    " ‚îú‚îÄ‚îÄ Partition 3 ‚Üí Executor 3\n",
    " ‚îî‚îÄ‚îÄ Partition 4 ‚Üí Executor 4\n",
    "```\n",
    "\n",
    "This is why Spark is fast ‚Äî parallel execution.\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Fault-Tolerant\n",
    "\n",
    "## üîπ What Does Fault-Tolerant Mean?\n",
    "\n",
    "If a partition is lost (executor crash), Spark can recompute it.\n",
    "\n",
    "How?\n",
    "\n",
    "üëâ Using **Lineage (DAG of transformations)**\n",
    "\n",
    "Spark does NOT replicate all data.  \n",
    "It remembers how the RDD was created.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example\n",
    "\n",
    "```python\n",
    "rdd1 = spark.sparkContext.parallelize([1,2,3,4])\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 4)\n",
    "\n",
    "print(rdd3.toDebugString())\n",
    "```\n",
    "\n",
    "Output shows lineage:\n",
    "\n",
    "```\n",
    "RDD3\n",
    "  |\n",
    "RDD2 (map)\n",
    "  |\n",
    "RDD1 (parallelize)\n",
    "```\n",
    "\n",
    "If partition of `rdd3` is lost:\n",
    "\n",
    "Spark recomputes:\n",
    "\n",
    "```\n",
    "parallelize ‚Üí map ‚Üí filter\n",
    "```\n",
    "\n",
    "That is fault tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ No Schema\n",
    "\n",
    "## üîπ What Does No Schema Mean?\n",
    "\n",
    "RDD does NOT enforce column names or data types.\n",
    "\n",
    "It is just a collection of objects.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\")\n",
    "])\n",
    "\n",
    "print(rdd.collect())\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "[(1, 'Alice'), (2, 'Bob')]\n",
    "```\n",
    "\n",
    "Spark does NOT know:\n",
    "\n",
    "- First column is \"id\"\n",
    "- Second column is \"name\"\n",
    "\n",
    "No schema metadata.\n",
    "\n",
    "Compare with DataFrame:\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame(rdd, [\"id\", \"name\"])\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- id: long\n",
    " |-- name: string\n",
    "```\n",
    "\n",
    "DataFrame has schema.  \n",
    "RDD does NOT.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ No Catalyst Optimization\n",
    "\n",
    "## üîπ What is Catalyst?\n",
    "\n",
    "Catalyst is Spark‚Äôs **query optimizer engine**.\n",
    "\n",
    "It optimizes:\n",
    "\n",
    "- Filter pushdown\n",
    "- Column pruning\n",
    "- Join reordering\n",
    "- Constant folding\n",
    "- Predicate simplification\n",
    "\n",
    "DataFrames use Catalyst.\n",
    "\n",
    "RDD does NOT.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example\n",
    "\n",
    "### RDD (No Optimization)\n",
    "\n",
    "```python\n",
    "rdd.filter(lambda x: x > 10).map(lambda x: x * 2)\n",
    "```\n",
    "\n",
    "Spark executes exactly as written.\n",
    "\n",
    "No optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### DataFrame (Optimized)\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 10\").select(\"name\")\n",
    "```\n",
    "\n",
    "Catalyst will:\n",
    "\n",
    "- Push filter early\n",
    "- Remove unused columns\n",
    "- Optimize execution plan\n",
    "\n",
    "You can see it:\n",
    "\n",
    "```python\n",
    "df.explain(True)\n",
    "```\n",
    "\n",
    "RDD does NOT support `.explain()`.\n",
    "\n",
    "---\n",
    "\n",
    "# üî• Interview-Level Comparison\n",
    "\n",
    "| Feature | RDD | DataFrame |\n",
    "|----------|------|------------|\n",
    "| Immutable | ‚úÖ | ‚úÖ |\n",
    "| Distributed | ‚úÖ | ‚úÖ |\n",
    "| Fault Tolerance | Lineage | Lineage |\n",
    "| Schema | ‚ùå No | ‚úÖ Yes |\n",
    "| Optimization | ‚ùå None | ‚úÖ Catalyst |\n",
    "| Performance | Slower | Faster |\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Final Interview Answer\n",
    "\n",
    "RDD is:\n",
    "\n",
    "- Immutable\n",
    "- Distributed across partitions\n",
    "- Fault-tolerant via lineage\n",
    "- Schema-less\n",
    "- Not optimized by Catalyst\n",
    "\n",
    "DataFrames are preferred in modern Spark because they are optimized and faster.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Key Concept to Remember\n",
    "\n",
    "RDD = Low-level API  \n",
    "DataFrame = Optimized Structured API  \n",
    "Catalyst = Spark SQL Optimization Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5e379452-371d-460d-a286-7acbc8b6417d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# RDD Hand-on and Practice Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "611152a2-9392-494c-b1be-b4e73c8e71c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4],3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ba6be576-c567-464c-a0b3-645f76967754",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# print(rdd.foreach(lambda x: x))\n",
    "\n",
    "rdd.collect()\n",
    "rdd.getNumPartitions()\n",
    "\n",
    "# What glom() does:\n",
    "# Converts each partition into a list\n",
    "# Then returns list of partitions\n",
    "\n",
    "rdd.glom().collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d46b56a4-80ab-4534-affe-52d4e6db056f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Lazy Evaluation and Actions in Apache Spark\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is Lazy Evaluation?\n",
    "\n",
    "Lazy Evaluation means:\n",
    "\n",
    "> Spark does NOT execute transformations immediately.  \n",
    "> It waits until an **Action** is called.\n",
    "\n",
    "Spark only builds a **logical execution plan (DAG)** during transformations.\n",
    "\n",
    "Actual computation happens only when an action triggers execution.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Why Spark Uses Lazy Evaluation?\n",
    "\n",
    "Lazy evaluation allows Spark to:\n",
    "\n",
    "- Optimize the entire query plan using Catalyst\n",
    "- Combine multiple transformations\n",
    "- Reduce unnecessary computations\n",
    "- Minimize shuffle operations\n",
    "- Improve performance\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Transformations vs Actions\n",
    "\n",
    "## üîπ Transformations\n",
    "\n",
    "Transformations are **lazy operations**.\n",
    "\n",
    "They:\n",
    "\n",
    "- Create a new RDD/DataFrame\n",
    "- Do NOT execute immediately\n",
    "- Build a DAG (execution plan)\n",
    "\n",
    "### Examples (RDD):\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 4)\n",
    "```\n",
    "\n",
    "Nothing runs yet.\n",
    "\n",
    "---\n",
    "\n",
    "### Examples (DataFrame):\n",
    "\n",
    "```python\n",
    "df2 = df.filter(\"age > 25\").select(\"name\")\n",
    "```\n",
    "\n",
    "Still nothing runs.\n",
    "\n",
    "Spark is only building a plan.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Actions\n",
    "\n",
    "Actions trigger execution.\n",
    "\n",
    "They:\n",
    "\n",
    "- Execute the DAG\n",
    "- Return results to driver\n",
    "- Write output to storage\n",
    "\n",
    "### Common Actions (RDD)\n",
    "\n",
    "- collect()\n",
    "- count()\n",
    "- first()\n",
    "- take()\n",
    "- saveAsTextFile()\n",
    "\n",
    "### Common Actions (DataFrame)\n",
    "\n",
    "- show()\n",
    "- count()\n",
    "- collect()\n",
    "- write()\n",
    "- foreach()\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Example: Lazy Evaluation in Action\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 4)\n",
    "\n",
    "print(\"Nothing executed yet\")\n",
    "\n",
    "result = rdd3.collect()\n",
    "print(result)\n",
    "```\n",
    "\n",
    "### Execution Flow:\n",
    "\n",
    "1. map() ‚Üí transformation (lazy)\n",
    "2. filter() ‚Üí transformation (lazy)\n",
    "3. collect() ‚Üí action (triggers execution)\n",
    "\n",
    "Only when `collect()` runs does Spark:\n",
    "\n",
    "- Create DAG\n",
    "- Split into stages\n",
    "- Schedule tasks\n",
    "- Execute on executors\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ How Lazy Evaluation Works Internally\n",
    "\n",
    "When transformations are written:\n",
    "\n",
    "Spark builds a **DAG (Directed Acyclic Graph)**.\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "parallelize ‚Üí map ‚Üí filter ‚Üí collect\n",
    "```\n",
    "\n",
    "Spark combines operations when possible before execution.\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Real Interview Concept: DAG Creation\n",
    "\n",
    "Each transformation is added as a node in DAG.\n",
    "\n",
    "Only at action time:\n",
    "\n",
    "1. DAG Scheduler splits into stages\n",
    "2. Stages split into tasks\n",
    "3. Tasks run on executors\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Performance Advantage of Lazy Evaluation\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 25\").select(\"name\").filter(\"name IS NOT NULL\")\n",
    "```\n",
    "\n",
    "Without lazy evaluation:\n",
    "\n",
    "- Each step would run separately.\n",
    "\n",
    "With lazy evaluation:\n",
    "\n",
    "- Spark combines filters.\n",
    "- Applies predicate pushdown.\n",
    "- Removes unused columns.\n",
    "- Optimizes execution.\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Example with DataFrame\n",
    "\n",
    "```python\n",
    "df2 = df.filter(\"age > 25\").select(\"name\")\n",
    "\n",
    "df2.show()\n",
    "```\n",
    "\n",
    "- filter() ‚Üí lazy\n",
    "- select() ‚Üí lazy\n",
    "- show() ‚Üí action ‚Üí triggers execution\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Types of Transformations\n",
    "\n",
    "## üîπ Narrow Transformations\n",
    "\n",
    "- map()\n",
    "- filter()\n",
    "- flatMap()\n",
    "\n",
    "No shuffle required.\n",
    "\n",
    "One partition ‚Üí One partition.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Wide Transformations\n",
    "\n",
    "- groupBy()\n",
    "- join()\n",
    "- distinct()\n",
    "- reduceByKey()\n",
    "\n",
    "Require shuffle.\n",
    "\n",
    "Creates new stage in DAG.\n",
    "\n",
    "---\n",
    "\n",
    "# üîü Important Interview Question\n",
    "\n",
    "## Q: Why is Spark faster than MapReduce?\n",
    "\n",
    "Because:\n",
    "\n",
    "- Lazy evaluation\n",
    "- In-memory processing\n",
    "- DAG optimization\n",
    "- Reduced disk writes\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ Example Showing Multiple Actions\n",
    "\n",
    "```python\n",
    "df2 = df.filter(\"age > 25\")\n",
    "\n",
    "df2.count()\n",
    "df2.show()\n",
    "```\n",
    "\n",
    "Important:\n",
    "\n",
    "Each action triggers execution separately.\n",
    "\n",
    "Unless cached:\n",
    "\n",
    "```python\n",
    "df2.cache()\n",
    "df2.count()\n",
    "df2.show()\n",
    "```\n",
    "\n",
    "Now data is computed once and reused.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£2Ô∏è‚É£ Interview-Level Definition\n",
    "\n",
    "Lazy Evaluation is Spark's execution strategy where transformations are not executed immediately but are recorded in a DAG and executed only when an action is triggered, allowing Spark to optimize the entire execution plan before running it.\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£3Ô∏è‚É£ Key Takeaways\n",
    "\n",
    "- Transformations are lazy.\n",
    "- Actions trigger execution.\n",
    "- Spark builds DAG.\n",
    "- Catalyst optimizes before execution.\n",
    "- Improves performance.\n",
    "- Reduces shuffle and I/O.\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Summary Table\n",
    "\n",
    "| Concept | Description |\n",
    "|----------|-------------|\n",
    "| Lazy Evaluation | Delayed execution until action |\n",
    "| Transformation | Lazy operation |\n",
    "| Action | Triggers execution |\n",
    "| DAG | Execution plan |\n",
    "| Optimization | Done before action |\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Must Remember for Interviews\n",
    "\n",
    "- Spark is lazy.\n",
    "- No computation happens without an action.\n",
    "- Multiple actions = multiple executions (unless cached).\n",
    "- Lazy evaluation enables Catalyst optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59bb2443-b25a-4f05-8479-29988c8efc81",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f3f45cb-1425-4c69-9c33-eb60f1245c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Lazy evaluation and Action vs Transformation\n",
    "# Transformations are lazy and only executed when an action is called.\n",
    "# Actions trigger the execution of transformations and return results.\n",
    "# df = spark.read.csv(\"/databricks-datasets/learning-spark-v2/people/people-10m.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dd3aed43-bae9-47fb-8e33-56660c6c8de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ Catalyst Optimizer in Apache Spark\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ What is Catalyst?\n",
    "\n",
    "Catalyst is Spark‚Äôs **query optimization engine**.\n",
    "\n",
    "It is part of **Spark SQL**.\n",
    "\n",
    "It automatically optimizes queries written using:\n",
    "\n",
    "- DataFrame API\n",
    "- SQL queries\n",
    "- Dataset API (Scala)\n",
    "\n",
    "RDD does NOT use Catalyst.\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ Why Catalyst Was Introduced?\n",
    "\n",
    "Before Spark 2.0:\n",
    "\n",
    "- RDD operations were executed as written.\n",
    "- No query optimization.\n",
    "- Performance depended heavily on user code.\n",
    "\n",
    "Catalyst was introduced to:\n",
    "\n",
    "- Automatically optimize execution plans\n",
    "- Improve performance\n",
    "- Reduce unnecessary data processing\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ Where Does Catalyst Work?\n",
    "\n",
    "When you run:\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 25\").select(\"name\")\n",
    "```\n",
    "\n",
    "Spark does NOT execute immediately.\n",
    "\n",
    "Instead:\n",
    "\n",
    "1. Builds Logical Plan\n",
    "2. Optimizes Logical Plan (Catalyst)\n",
    "3. Converts to Physical Plan\n",
    "4. Executes\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Catalyst Optimization Flow\n",
    "\n",
    "```\n",
    "User Query\n",
    "    ‚Üì\n",
    "Unresolved Logical Plan\n",
    "    ‚Üì\n",
    "Analyzed Logical Plan\n",
    "    ‚Üì\n",
    "Optimized Logical Plan   ‚Üê Catalyst Works Here\n",
    "    ‚Üì\n",
    "Physical Plan\n",
    "    ‚Üì\n",
    "Execution\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ What Does Catalyst Actually Do?\n",
    "\n",
    "Catalyst performs multiple rule-based optimizations.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 1. Predicate Pushdown\n",
    "\n",
    "Move filters as early as possible.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 25\").select(\"name\")\n",
    "```\n",
    "\n",
    "Instead of:\n",
    "\n",
    "1. Read all columns\n",
    "2. Then filter\n",
    "\n",
    "Catalyst:\n",
    "\n",
    "- Reads only required column\n",
    "- Applies filter early\n",
    "\n",
    "# Predicate Pushdown in Apache Spark\n",
    "\n",
    "## What is Predicate Pushdown?\n",
    "\n",
    "Predicate pushdown is an optimization technique where Spark pushes filter conditions down to the data source level.  \n",
    "This allows Spark to read only the required rows instead of scanning the entire dataset.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "1. Read full dataset  \n",
    "2. Apply filter  \n",
    "\n",
    "Spark optimizes it to:\n",
    "\n",
    "1. Apply filter during data read  \n",
    "2. Read only matching rows  \n",
    "\n",
    "---\n",
    "\n",
    "## Example Query\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 25\").select(\"name\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Using `explain(True)`\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 25\").select(\"name\").explain(True)\n",
    "```\n",
    "\n",
    "This does **not** change the query.\n",
    "\n",
    "It only prints:\n",
    "\n",
    "- Parsed Logical Plan  \n",
    "- Analyzed Logical Plan  \n",
    "- Optimized Logical Plan  \n",
    "- Physical Plan  \n",
    "\n",
    "---\n",
    "\n",
    "## Optimized Execution Plan (Internal)\n",
    "\n",
    "Spark rewrites the query internally to something like:\n",
    "\n",
    "```\n",
    "Project [name]\n",
    "  ‚îî‚îÄ‚îÄ FileScan parquet\n",
    "        PushedFilters: [GreaterThan(age,25)]\n",
    "        ReadSchema: struct<name:string, age:int>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Optimizations Happening\n",
    "\n",
    "### 1. Predicate Pushdown\n",
    "The filter `age > 25` is pushed down to the storage layer.\n",
    "\n",
    "### 2. Column Pruning\n",
    "Only required columns (`name`, `age`) are read from disk.\n",
    "\n",
    "---\n",
    "\n",
    "## Why It Is Faster\n",
    "\n",
    "- Less data read from disk  \n",
    "- Reduced network I/O  \n",
    "- Lower memory usage  \n",
    "- Faster execution  \n",
    "\n",
    "---\n",
    "\n",
    "## Important Notes\n",
    "\n",
    "- Spark uses **lazy evaluation**. Transformations are not executed until an action is called (`show()`, `count()`, `write()`, etc.).\n",
    "- `.explain(True)` only displays the execution plan.\n",
    "- Predicate pushdown works best with:\n",
    "  - Parquet  \n",
    "  - ORC  \n",
    "  - JDBC (depends on driver)\n",
    "- It may not fully work with CSV or text-based sources.\n",
    "\n",
    "---\n",
    "\n",
    "## Final Takeaway\n",
    "\n",
    "Your code:\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 25\").select(\"name\")\n",
    "```\n",
    "\n",
    "Spark automatically optimizes it before execution using the Catalyst Optimizer.\n",
    "\n",
    "You write simple transformations ‚Äî Spark handles the optimization internally.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 2. Column Pruning\n",
    "\n",
    "Remove unused columns.\n",
    "\n",
    "If you write:\n",
    "\n",
    "```python\n",
    "df.select(\"name\")\n",
    "```\n",
    "\n",
    "Spark will only read \"name\" column from storage.\n",
    "\n",
    "Other columns are ignored.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 3. Constant Folding\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df.filter(\"salary > 1000 + 2000\")\n",
    "```\n",
    "\n",
    "Catalyst converts:\n",
    "\n",
    "```\n",
    "1000 + 2000 ‚Üí 3000\n",
    "```\n",
    "\n",
    "Before execution.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 4. Join Reordering\n",
    "\n",
    "If multiple joins exist:\n",
    "\n",
    "Catalyst changes join order to reduce shuffle cost.\n",
    "\n",
    "\n",
    "# Join Reordering in Spark Catalyst Optimizer\n",
    "\n",
    "## üîπ What is Join Reordering?\n",
    "\n",
    "Join Reordering is an optimization where Spark's Catalyst Optimizer changes the order of joins \n",
    "to reduce shuffle cost and improve performance.\n",
    "\n",
    "When multiple joins exist, the order in which they are executed can significantly impact performance.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Why Join Order Matters\n",
    "\n",
    "Joins usually cause:\n",
    "\n",
    "- Data shuffling across partitions\n",
    "- Network I/O\n",
    "- Memory usage increase\n",
    "\n",
    "If large tables are joined first, shuffle size becomes huge.\n",
    "\n",
    "If smaller tables are joined first, intermediate results stay smaller, reducing cost.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example\n",
    "\n",
    "```python\n",
    "result = A.join(B, \"id\") \\\n",
    "          .join(C, \"id\") \\\n",
    "          .join(D, \"id\")\n",
    "```\n",
    "\n",
    "You wrote joins in this order:\n",
    "\n",
    "1. A ‚ãà B\n",
    "2. (A ‚ãà B) ‚ãà C\n",
    "3. (...) ‚ãà D\n",
    "\n",
    "But Catalyst may change the order internally.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How Catalyst Decides Join Order\n",
    "\n",
    "Catalyst uses:\n",
    "\n",
    "### 1Ô∏è‚É£ Cost-Based Optimization (CBO)\n",
    "\n",
    "If table statistics are available (row count, size, etc.), Spark estimates:\n",
    "\n",
    "- Output size of each join\n",
    "- Shuffle cost\n",
    "- Memory usage\n",
    "\n",
    "Then it chooses the lowest-cost join plan.\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Star Schema Detection\n",
    "\n",
    "If one large fact table joins multiple small dimension tables:\n",
    "\n",
    "Fact ‚ãà Dim1 ‚ãà Dim2 ‚ãà Dim3\n",
    "\n",
    "Spark may:\n",
    "\n",
    "- Broadcast smaller dimension tables\n",
    "- Join them first\n",
    "- Reduce shuffle\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example Scenario\n",
    "\n",
    "Suppose:\n",
    "\n",
    "- A = 1 billion rows\n",
    "- B = 10 million rows\n",
    "- C = 1 thousand rows\n",
    "\n",
    "You wrote:\n",
    "\n",
    "A ‚ãà B ‚ãà C\n",
    "\n",
    "Catalyst may reorder it to:\n",
    "\n",
    "B ‚ãà C first  \n",
    "Then join with A  \n",
    "\n",
    "Because:\n",
    "\n",
    "(B ‚ãà C) produces much smaller intermediate result.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Logical vs Physical Plan\n",
    "\n",
    "Your Code:\n",
    "\n",
    "```python\n",
    "A.join(B, \"id\").join(C, \"id\")\n",
    "```\n",
    "\n",
    "Optimized Logical Plan (after reordering):\n",
    "\n",
    "```\n",
    "Join\n",
    " ‚îú‚îÄ‚îÄ A\n",
    " ‚îî‚îÄ‚îÄ Join\n",
    "      ‚îú‚îÄ‚îÄ B\n",
    "      ‚îî‚îÄ‚îÄ C\n",
    "```\n",
    "\n",
    "Join order changed internally.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ When Join Reordering Works\n",
    "\n",
    "- Multiple joins present\n",
    "- Cost-Based Optimizer enabled\n",
    "- Table statistics available\n",
    "- spark.sql.cbo.enabled = true\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ When It Does NOT Reorder\n",
    "\n",
    "- Statistics are missing\n",
    "- Join hints are provided (e.g., broadcast hint)\n",
    "- Complex join conditions prevent optimization\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ How to Enable Cost-Based Optimization\n",
    "\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.cbo.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.statistics.histogram.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "And collect stats:\n",
    "\n",
    "```sql\n",
    "ANALYZE TABLE table_name COMPUTE STATISTICS;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Final Takeaway\n",
    "\n",
    "You may write:\n",
    "\n",
    "A ‚ãà B ‚ãà C\n",
    "\n",
    "But Catalyst may execute:\n",
    "\n",
    "(B ‚ãà C) ‚ãà A\n",
    "\n",
    "Join order is optimized automatically to reduce shuffle and execution cost.\n",
    "\n",
    "You write logical joins.  \n",
    "Catalyst decides the most efficient physical execution plan.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ 5. Filter Simplification\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "age > 25 AND age > 20\n",
    "```\n",
    "\n",
    "Catalyst simplifies to:\n",
    "\n",
    "```\n",
    "age > 25\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ How To See Catalyst in Action?\n",
    "\n",
    "Use:\n",
    "\n",
    "```python\n",
    "df.explain(True)\n",
    "```\n",
    "\n",
    "You will see:\n",
    "\n",
    "- Parsed Logical Plan\n",
    "- Analyzed Logical Plan\n",
    "- Optimized Logical Plan\n",
    "- Physical Plan\n",
    "\n",
    "Example output:\n",
    "\n",
    "```\n",
    "== Optimized Logical Plan ==\n",
    "Project [name#10]\n",
    "+- Filter (age#9 > 25)\n",
    "```\n",
    "\n",
    "That optimized plan is created by Catalyst.\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ Why RDD Does NOT Use Catalyst?\n",
    "\n",
    "RDD is:\n",
    "\n",
    "- Low-level API\n",
    "- Functional transformations\n",
    "- No schema\n",
    "- No structured query plan\n",
    "\n",
    "Catalyst needs schema and structured plan to optimize.\n",
    "\n",
    "Since RDD has no schema ‚Üí no optimization possible.\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Why Catalyst Makes DataFrame Faster Than RDD\n",
    "\n",
    "RDD:\n",
    "\n",
    "- Executes transformations exactly as written\n",
    "- No reordering\n",
    "- No pushdown\n",
    "- More memory usage\n",
    "\n",
    "DataFrame:\n",
    "\n",
    "- Optimized query plan\n",
    "- Reduced I/O\n",
    "- Reduced shuffle\n",
    "- Better execution strategy\n",
    "\n",
    "Result:\n",
    "\n",
    "DataFrame is significantly faster.\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ What Is Tungsten? (Related to Catalyst)\n",
    "\n",
    "Catalyst = Query optimizer  \n",
    "Tungsten = Execution engine\n",
    "\n",
    "Tungsten improves:\n",
    "\n",
    "- Memory management\n",
    "- CPU usage\n",
    "- Binary processing\n",
    "- Code generation\n",
    "\n",
    "Catalyst optimizes plan ‚Üí Tungsten executes efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "# üî• Interview-Ready Definition\n",
    "\n",
    "Catalyst is Spark SQL‚Äôs rule-based query optimizer that transforms logical plans into optimized execution plans to improve performance by applying techniques like predicate pushdown, column pruning, constant folding, and join reordering.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Quick Summary\n",
    "\n",
    "- Catalyst = Query Optimizer\n",
    "- Works only with DataFrame / SQL / Dataset\n",
    "- Not used in RDD\n",
    "- Improves performance automatically\n",
    "- Visible using df.explain(True)\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Important for Data Engineers\n",
    "\n",
    "Understanding Catalyst helps in:\n",
    "\n",
    "- Writing efficient Spark queries\n",
    "- Reducing shuffle\n",
    "- Optimizing joins\n",
    "- Debugging performance issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9f483171-5490-4713-a23e-70b64d5fcc36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üîπ Directed Acyclic Graph (DAG) in Apache Spark\n",
    "\n",
    "## 1Ô∏è‚É£ What is a DAG?\n",
    "\n",
    "A **Directed Acyclic Graph (DAG)** is a **graph-based representation of computation** in Spark where:\n",
    "\n",
    "- **Nodes** represent **RDDs or DataFrames** (data or intermediate datasets)\n",
    "- **Edges** represent **transformations** (map, filter, join, etc.)\n",
    "- **Directed** ‚Üí edges have a direction (from parent to child)\n",
    "- **Acyclic** ‚Üí there are **no loops or cycles**\n",
    "\n",
    "Essentially, a DAG shows **how data flows from source to final output**.\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Why DAG Matters in Spark\n",
    "\n",
    "Spark uses DAGs to:\n",
    "\n",
    "1. Plan execution efficiently\n",
    "2. Track **lineage** of transformations\n",
    "3. Optimize execution (e.g., **pipelining transformations**)\n",
    "4. Recover lost data in case of failure (fault tolerance)\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Example 1: Simple Transformations\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 5)\n",
    "rdd3.collect()\n",
    "```\n",
    "\n",
    "### DAG Representation\n",
    "\n",
    "```\n",
    "[Input RDD] --> map(lambda x: x*2) --> filter(lambda x > 5) --> [Output RDD]\n",
    "```\n",
    "\n",
    "- Node 1 ‚Üí `[1,2,3,4,5]`\n",
    "- Node 2 ‚Üí `[2,4,6,8,10]` (after map)\n",
    "- Node 3 ‚Üí `[6,8,10]` (after filter)\n",
    "- Edges ‚Üí transformations applied in sequence\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Example 2: Multiple Dependencies\n",
    "\n",
    "```python\n",
    "rdd1 = spark.sparkContext.parallelize([1,2,3])\n",
    "rdd2 = spark.sparkContext.parallelize([4,5,6])\n",
    "rdd3 = rdd1.union(rdd2)\n",
    "rdd4 = rdd3.map(lambda x: x * 10)\n",
    "rdd4.collect()\n",
    "```\n",
    "\n",
    "### DAG Representation\n",
    "\n",
    "```\n",
    "rdd1 [1,2,3]  ----\\\n",
    "                     union --> map(lambda x*10) --> rdd4 [10,20,30,40,50,60]\n",
    "rdd2 [4,5,6]  ----/\n",
    "```\n",
    "\n",
    "- `rdd4` depends on **both rdd1 and rdd2**\n",
    "- DAG captures **multiple parent nodes**\n",
    "- Spark executes transformations in stages based on DAG\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ DAG vs Execution Plan\n",
    "\n",
    "- Transformations like `map`, `filter`, `union` are **lazy**\n",
    "- DAG is **built first** (logical plan)\n",
    "- When an **action** (`collect`, `count`, `save`) is called, Spark:\n",
    "  1. Divides DAG into **stages**\n",
    "  2. Executes **tasks in parallel**\n",
    "  3. Optimizes **shuffles and pipelining**\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ DAG in Spark UI\n",
    "\n",
    "- Each job in Spark UI shows a DAG\n",
    "- **Stages** ‚Üí separated by **shuffle boundaries**\n",
    "- **Tasks** ‚Üí units of work executed per partition\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Example with Stage Separation (Shuffle)\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.reduceByKey(lambda a,b: a+b)   # triggers shuffle\n",
    "rdd3.collect()\n",
    "```\n",
    "\n",
    "- DAG splits into **two stages**:\n",
    "  1. Map stage ‚Üí `map(lambda x: x*2)`\n",
    "  2. Reduce stage ‚Üí `reduceByKey` (shuffle required)\n",
    "- DAG ensures **minimal stages** and **parallelism**\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Key Points\n",
    "\n",
    "- DAG is **core to Spark‚Äôs fault tolerance**\n",
    "- Spark rebuilds lost partitions using DAG lineage\n",
    "- DAG allows **pipelining transformations** in a single stage when no shuffle is needed\n",
    "- Actions trigger DAG execution; transformations only define DAG\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Summary\n",
    "\n",
    "- DAG = logical flow of computations  \n",
    "- Nodes = RDDs / DataFrames  \n",
    "- Edges = transformations  \n",
    "- Supports **lazy evaluation**, **parallel execution**, and **fault tolerance**  \n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "[Read CSV] -> filter(age>25) -> select(name) -> groupBy(city) -> save()\n",
    "```\n",
    "\n",
    "This is your DAG in real Spark jobs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "68241012-38db-4ec1-86c6-fcb261800ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "%md\n",
    "# üî• Directed Acyclic Graph (DAG) in Apache Spark ‚Äì Interview Master Guide\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ What is a DAG?\n",
    "\n",
    "> **In Spark, a DAG (Directed Acyclic Graph) is a logical execution plan built by the DAG Scheduler inside the Driver. It represents RDD/DataFrame transformations as nodes and dependencies as edges. Spark divides the DAG into stages based on shuffle boundaries and executes them as tasks across executors, ensuring parallelism, pipelining, and fault tolerance using lineage.**\n",
    "\n",
    "A **Directed Acyclic Graph (DAG)** in Apache Spark is a **logical execution plan** that represents:\n",
    "\n",
    "- **Nodes** represent **RDDs or DataFrames** (data or intermediate datasets)\n",
    "- **Edges** represent **transformations** (map, filter, join, etc.)\n",
    "- **Directed** ‚Üí edges have a direction (from parent to child)\n",
    "- **Acyclic** ‚Üí there are **no loops or cycles**\n",
    "\n",
    "Spark builds this graph **lazily to understand how data flows from source to final output.**\n",
    "Essentially, a DAG shows **how data flows from source to final output**.\n",
    "\n",
    "\n",
    "**Why DAG Matters in Spark**\n",
    "\n",
    "Spark uses DAGs to:\n",
    "\n",
    "1. Plan execution efficiently\n",
    "2. Track **lineage** of transformations\n",
    "3. Optimize execution (e.g., **pipelining transformations**)\n",
    "4. Recover lost data in case of failure (fault tolerance)\n",
    "\n",
    "\n",
    "**Example 1: Simple Transformations**\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])\n",
    "rdd2 = rdd.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 5)\n",
    "rdd3.collect()\n",
    "```\n",
    "\n",
    "### DAG Representation\n",
    "\n",
    "```\n",
    "[Input RDD] --> map(lambda x: x*2) --> filter(lambda x > 5) --> [Output RDD]\n",
    "```\n",
    "\n",
    "- Node 1 ‚Üí `[1,2,3,4,5]`\n",
    "- Node 2 ‚Üí `[2,4,6,8,10]` (after map)\n",
    "- Node 3 ‚Üí `[6,8,10]` (after filter)\n",
    "- Edges ‚Üí transformations applied in sequence\n",
    "\n",
    "**Example 2: Multiple Dependencies**\n",
    "\n",
    "```python\n",
    "rdd1 = spark.sparkContext.parallelize([1,2,3])\n",
    "rdd2 = spark.sparkContext.parallelize([4,5,6])\n",
    "rdd3 = rdd1.union(rdd2)\n",
    "rdd4 = rdd3.map(lambda x: x * 10)\n",
    "rdd4.collect()\n",
    "```\n",
    "\n",
    "### DAG Representation\n",
    "\n",
    "```\n",
    "rdd1 [1,2,3]  ----\\\n",
    "                    union --> map(lambda x*10) --> rdd4 [10,20,30,40,50,60]\n",
    "rdd2 [4,5,6]  ----/\n",
    "```\n",
    "\n",
    "- `rdd4` depends on **both rdd1 and rdd2**\n",
    "- DAG captures **multiple parent nodes**\n",
    "- Spark executes transformations in stages based on DAG\n",
    "\n",
    "**DAG vs Execution Plan in Spark**\n",
    "\n",
    "| Concept | What it is | When it exists | Key Points |\n",
    "|---------|------------|---------------|------------|\n",
    "| **DAG (Directed Acyclic Graph)** | Logical representation of all transformations you wrote | Built as soon as you define transformations, before any action | Shows how data flows from sources through transformations to output. Think of it as the recipe. |\n",
    "| **Execution Plan** | Physical plan showing how Spark will actually execute the DAG | Created when an action is called | Divides DAG into stages, assigns tasks to partitions, handles shuffles, optimizations, pipelining, and parallel execution. |\n",
    "\n",
    "**How They Relate**\n",
    "\n",
    "- You define transformations (`map`, `filter`, `join`) ‚Üí **DAG is built automatically**\n",
    "- When you call an action (`collect`, `count`, `save`) ‚Üí **Spark generates an execution plan from the DAG**\n",
    "- The **execution plan** is essentially a **concrete version of the DAG** optimized for parallel execution\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "- **DAG** = blueprint / recipe  \n",
    "- **Execution Plan** = kitchen workflow, telling which cook does what, in what order, and what utensils to use\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ Where DAG Exists in Spark Architecture?\n",
    "\n",
    "Inside the **Driver Program**, Spark has:\n",
    "\n",
    "- **SparkContext / SparkSession**\n",
    "- **DAG Scheduler**\n",
    "- **Task Scheduler**\n",
    "- **Cluster Manager**\n",
    "- **Executors**\n",
    "\n",
    "üëâ The DAG is created inside the **Driver** and handled by the **DAG Scheduler**.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ How DAG is Created\n",
    "\n",
    "### Step 1: User Defines Transformations\n",
    "\n",
    "```python\n",
    "rdd = sc.textFile(\"data.txt\")\n",
    "rdd2 = rdd.map(lambda x: x.split(\",\"))\n",
    "rdd3 = rdd2.filter(lambda x: x[1] == \"India\")\n",
    "```\n",
    "\n",
    "### Step 2: Lazy Evaluation\n",
    "\n",
    "Spark does NOT execute transformations immediately.\n",
    "\n",
    "Instead:\n",
    "- It records them in a **lineage graph**\n",
    "- Builds a DAG\n",
    "- Waits for an **action**\n",
    "\n",
    "### Step 3: Action Triggered\n",
    "\n",
    "```python\n",
    "rdd3.collect()\n",
    "```\n",
    "\n",
    "When an action is called:\n",
    "- DAG is finalized\n",
    "- Divided into **stages**\n",
    "- Tasks are created (1 per partition)\n",
    "- Sent to executors\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Narrow vs Wide Dependencies (Very Important for Interviews)\n",
    "\n",
    "### üîπ Narrow Dependency\n",
    "\n",
    "- Each child partition depends on only one parent partition\n",
    "- No shuffle required\n",
    "- Transformations are pipelined\n",
    "\n",
    "Examples:\n",
    "- `map()`\n",
    "- `filter()`\n",
    "- `flatMap()`\n",
    "\n",
    "Diagram:\n",
    "\n",
    "```\n",
    "Partition 1 ‚Üí Partition 1\n",
    "Partition 2 ‚Üí Partition 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîπ Wide Dependency\n",
    "\n",
    "- Child partition depends on multiple parent partitions\n",
    "- Shuffle required\n",
    "- Creates stage boundary\n",
    "\n",
    "Examples:\n",
    "- `reduceByKey()`\n",
    "- `groupByKey()`\n",
    "- `join()`\n",
    "- `distinct()`\n",
    "\n",
    "Diagram:\n",
    "\n",
    "```\n",
    "Partition 1 ‚Üí All partitions\n",
    "Partition 2 ‚Üí All partitions\n",
    "```\n",
    "\n",
    "üëâ Wide dependency = New Stage\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Stage Creation in DAG\n",
    "\n",
    "Spark divides DAG into stages based on shuffle boundaries.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "rdd = sc.textFile(\"file.txt\")\n",
    "rdd2 = rdd.map(lambda x: x.split(\",\"))\n",
    "rdd3 = rdd2.reduceByKey(lambda a,b: a+b)\n",
    "rdd4 = rdd3.filter(lambda x: x[1] > 10)\n",
    "rdd4.collect()\n",
    "```\n",
    "\n",
    "Execution Plan:\n",
    "\n",
    "```\n",
    "Stage 1:\n",
    "textFile ‚Üí map\n",
    "\n",
    "(Shuffle Boundary)\n",
    "\n",
    "Stage 2:\n",
    "reduceByKey ‚Üí filter ‚Üí collect\n",
    "```\n",
    "\n",
    "- Stage 1 ‚Üí Narrow transformations\n",
    "- Stage 2 ‚Üí After shuffle\n",
    "\n",
    "Spark minimizes number of stages for performance.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ DAG Scheduler vs Task Scheduler\n",
    "\n",
    "### üîπ DAG Scheduler\n",
    "\n",
    "- Converts logical DAG ‚Üí Stages\n",
    "- Identifies shuffle boundaries\n",
    "- Creates stage graph\n",
    "\n",
    "### üîπ Task Scheduler\n",
    "\n",
    "- Takes each stage\n",
    "- Breaks into tasks (1 task per partition)\n",
    "- Assigns tasks to executors\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Fault Tolerance Using DAG (Lineage Concept)\n",
    "\n",
    "Spark does NOT replicate data like Hadoop.\n",
    "\n",
    "Instead:\n",
    "\n",
    "If a partition is lost:\n",
    "- Spark uses DAG lineage\n",
    "- Recomputes only lost partition\n",
    "- Re-executes transformations from source\n",
    "\n",
    "Example:\n",
    "\n",
    "```\n",
    "RDD1 ‚Üí map ‚Üí filter ‚Üí reduceByKey\n",
    "```\n",
    "\n",
    "If partition 3 is lost:\n",
    "- Spark recomputes only required lineage\n",
    "- Not entire dataset\n",
    "\n",
    "üëâ This makes Spark memory efficient.\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ DAG in RDD vs DataFrame\n",
    "\n",
    "### üîπ RDD\n",
    "\n",
    "- Simple lineage graph\n",
    "- No cost-based optimization\n",
    "\n",
    "### üîπ DataFrame\n",
    "\n",
    "Goes through **Catalyst Optimizer**\n",
    "\n",
    "Steps:\n",
    "\n",
    "1. Logical Plan\n",
    "2. Analyzed Logical Plan\n",
    "3. Optimized Logical Plan\n",
    "4. Physical Plan\n",
    "5. Whole Stage Code Generation\n",
    "\n",
    "RDD DAG ‚Üí Simple transformation graph  \n",
    "DataFrame DAG ‚Üí Optimized execution plan  \n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Logical Plan vs Physical Plan\n",
    "\n",
    "### Logical Plan ‚Üí \"What to do\"\n",
    "\n",
    "```\n",
    "Filter ‚Üí Project ‚Üí Aggregate\n",
    "```\n",
    "\n",
    "### Physical Plan ‚Üí \"How to do\"\n",
    "\n",
    "```\n",
    "HashAggregateExec\n",
    "SortMergeJoinExec\n",
    "BroadcastHashJoinExec\n",
    "```\n",
    "\n",
    "Spark chooses best plan using:\n",
    "- Statistics\n",
    "- Cost model\n",
    "- Available memory\n",
    "\n",
    "---\n",
    "\n",
    "## üîü DAG in Spark UI\n",
    "\n",
    "In Spark UI ‚Üí Jobs tab ‚Üí DAG visualization\n",
    "\n",
    "You will see:\n",
    "- Jobs\n",
    "- Stages\n",
    "- Tasks\n",
    "- Shuffle read/write\n",
    "- Execution time\n",
    "\n",
    "üëâ Each shuffle creates new stage.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£1Ô∏è‚É£ Job vs Stage vs Task (Very Important)\n",
    "\n",
    "| Level | Meaning |\n",
    "|-------|----------|\n",
    "| Job | Triggered by an Action |\n",
    "| Stage | Group of transformations without shuffle |\n",
    "| Task | Smallest unit of execution (per partition) |\n",
    "\n",
    "One action = One Job  \n",
    "Multiple shuffles = Multiple stages  \n",
    "Each partition = One task  \n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£2Ô∏è‚É£ Transformation Pipelining\n",
    "\n",
    "Spark pipelines narrow transformations.\n",
    "\n",
    "Instead of:\n",
    "\n",
    "```\n",
    "map ‚Üí filter ‚Üí map\n",
    "```\n",
    "\n",
    "Spark executes them in a single pass per partition.\n",
    "\n",
    "This improves:\n",
    "- CPU efficiency\n",
    "- Memory usage\n",
    "- Performance\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£3Ô∏è‚É£ Complete Execution Flow\n",
    "\n",
    "```\n",
    "User Code\n",
    "   ‚Üì\n",
    "Transformations (Lazy)\n",
    "   ‚Üì\n",
    "DAG Created\n",
    "   ‚Üì\n",
    "Action Triggered\n",
    "   ‚Üì\n",
    "DAG Scheduler ‚Üí Stages\n",
    "   ‚Üì\n",
    "Task Scheduler ‚Üí Tasks\n",
    "   ‚Üì\n",
    "Executors Execute\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£4Ô∏è‚É£ Why Spark Uses DAG Instead of MapReduce?\n",
    "\n",
    "MapReduce:\n",
    "- Fixed Map ‚Üí Reduce phases\n",
    "- Disk-heavy\n",
    "- Limited optimization\n",
    "\n",
    "Spark:\n",
    "- Arbitrary DAG\n",
    "- Multiple transformations\n",
    "- In-memory processing\n",
    "- Better optimization\n",
    "- Reduced disk I/O\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£5Ô∏è‚É£ Common Interview Questions\n",
    "\n",
    "### ‚ùì What creates a stage?\n",
    "Answer: Shuffle (wide dependency)\n",
    "\n",
    "### ‚ùì Does action create stage?\n",
    "No. Action creates a Job. Shuffle creates stages.\n",
    "\n",
    "### ‚ùì Can one program have multiple DAGs?\n",
    "Yes. Each action triggers a new job and DAG.\n",
    "\n",
    "### ‚ùì How does Spark recover from failure?\n",
    "Using lineage in DAG.\n",
    "\n",
    "---\n",
    "\n",
    "## üî• Interview-Level Definition (Memorize)\n",
    "\n",
    "> In Spark, a DAG (Directed Acyclic Graph) is a logical execution plan built by the DAG Scheduler inside the Driver. It represents RDD/DataFrame transformations as nodes and dependencies as edges. Spark divides the DAG into stages based on shuffle boundaries and executes them as tasks across executors, ensuring parallelism, pipelining, and fault tolerance using lineage.\n",
    "\n",
    "---\n",
    "\n",
    "# ‚úÖ Final Checklist\n",
    "\n",
    "‚úî DAG definition  \n",
    "‚úî Where DAG is created  \n",
    "‚úî Narrow vs Wide dependencies  \n",
    "‚úî Stage creation logic  \n",
    "‚úî DAG Scheduler role  \n",
    "‚úî Task Scheduler role  \n",
    "‚úî Fault tolerance via lineage  \n",
    "‚úî RDD vs DataFrame difference  \n",
    "‚úî Logical vs Physical plan  \n",
    "‚úî Job vs Stage vs Task  \n",
    "‚úî Spark UI understanding  \n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ Next Deep Dive Options\n",
    "\n",
    "- Shuffle Internals\n",
    "- Catalyst Optimizer Deep Dive\n",
    "- Tungsten Engine\n",
    "- Data Skew and Partitioning\n",
    "- Adaptive Query Execution (AQE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b961251a-6371-4f87-9438-fc713bfdef90",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spark read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "438c0785-8945-4c26-94c3-624b897470ec",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.read.format()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Spark Session & Spark Context 2026-02-17",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
