{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba89501f-3d0e-43fe-af9d-01d825d57c7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---\n",
    "\n",
    "# 1Ô∏è‚É£1Ô∏è‚É£ SparkContext (Very Important)\n",
    "\n",
    "## What is SparkContext?\n",
    "\n",
    "SparkContext is the entry point to Spark functionality.\n",
    "\n",
    "It represents:\n",
    "\n",
    "- Connection to cluster\n",
    "- Configuration of application\n",
    "- Resource coordination\n",
    "\n",
    "In older versions:\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp\")\n",
    "```\n",
    "\n",
    "In modern Spark:\n",
    "\n",
    "SparkSession internally creates SparkContext.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Responsibilities of SparkContext\n",
    "\n",
    "- Connects to cluster manager\n",
    "- Requests executors\n",
    "- Creates RDDs\n",
    "- Tracks application metadata\n",
    "- Distributes tasks\n",
    "- Manages broadcast variables\n",
    "- Manages accumulators\n",
    "\n",
    "---\n",
    "\n",
    "## SparkContext Architecture View\n",
    "\n",
    "```\n",
    "Application Code\n",
    "       ‚Üì\n",
    "SparkSession\n",
    "       ‚Üì\n",
    "SparkContext\n",
    "       ‚Üì\n",
    "Cluster Manager\n",
    "       ‚Üì\n",
    "Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Important SparkContext Concepts\n",
    "\n",
    "### 1Ô∏è‚É£ Broadcast Variables\n",
    "\n",
    "Used to send large read-only data to executors efficiently.\n",
    "\n",
    "```python\n",
    "broadcast_var = sc.broadcast(large_lookup_dict)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 2Ô∏è‚É£ Accumulators\n",
    "\n",
    "Used for counters across executors.\n",
    "\n",
    "```python\n",
    "counter = sc.accumulator(0)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3Ô∏è‚É£ Only One SparkContext Per JVM\n",
    "\n",
    "You cannot create multiple SparkContexts in the same application.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "899b7390-a8fa-4b9a-8301-6ad71dbd0a2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# üöÄ SparkSession vs SparkContext ‚Äî Detailed Interview Guide\n",
    "\n",
    "---\n",
    "\n",
    "# 1Ô∏è‚É£ Quick Summary\n",
    "\n",
    "| Feature | SparkContext | SparkSession |\n",
    "|----------|--------------|--------------|\n",
    "| Introduced In | Spark 1.x | Spark 2.x |\n",
    "| Purpose | Core Spark connection to cluster | Unified entry point to Spark |\n",
    "| API Type | RDD-based | DataFrame / SQL / Streaming |\n",
    "| Needed Today? | Yes (internally) | Yes (primary interface) |\n",
    "| Replaces | ‚Äî | SQLContext, HiveContext, SparkContext (partial) |\n",
    "\n",
    "---\n",
    "\n",
    "# 2Ô∏è‚É£ What is SparkContext?\n",
    "\n",
    "SparkContext is the **original entry point** to Spark (before Spark 2.0).\n",
    "\n",
    "It represents:\n",
    "\n",
    "- Connection to cluster\n",
    "- Resource coordination\n",
    "- RDD creation\n",
    "- Task scheduling\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ SparkContext Responsibilities\n",
    "\n",
    "- Connects to Cluster Manager\n",
    "- Requests executors\n",
    "- Creates RDDs\n",
    "- Distributes tasks\n",
    "- Manages broadcast variables\n",
    "- Manages accumulators\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example (Old Style)\n",
    "\n",
    "```python\n",
    "from pyspark import SparkContext\n",
    "\n",
    "sc = SparkContext(appName=\"MyApp\")\n",
    "\n",
    "rdd = sc.textFile(\"data.txt\")\n",
    "rdd.count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Important Facts\n",
    "\n",
    "- Only **one SparkContext per JVM**\n",
    "- If SparkContext stops ‚Üí Application ends\n",
    "- Core object behind everything\n",
    "\n",
    "---\n",
    "\n",
    "# 3Ô∏è‚É£ What is SparkSession?\n",
    "\n",
    "SparkSession was introduced in Spark 2.0.\n",
    "\n",
    "It is a **unified entry point** for:\n",
    "\n",
    "- Spark SQL\n",
    "- DataFrame API\n",
    "- Structured Streaming\n",
    "- Hive support\n",
    "\n",
    "It internally contains:\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ Example (Modern Way)\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MyApp\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"data.csv\")\n",
    "df.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 4Ô∏è‚É£ Relationship Between SparkSession and SparkContext\n",
    "\n",
    "Very important for interviews üëá\n",
    "\n",
    "```\n",
    "SparkSession\n",
    "     |\n",
    "     ‚îî‚îÄ‚îÄ SparkContext\n",
    "```\n",
    "\n",
    "SparkSession internally creates SparkContext.\n",
    "\n",
    "You can access it like this:\n",
    "\n",
    "```python\n",
    "sc = spark.sparkContext\n",
    "```\n",
    "\n",
    "So:\n",
    "\n",
    "> SparkSession is a wrapper around SparkContext.\n",
    "\n",
    "---\n",
    "\n",
    "# 5Ô∏è‚É£ Internal Architecture View\n",
    "\n",
    "```\n",
    "Application Code\n",
    "       ‚Üì\n",
    "SparkSession\n",
    "       ‚Üì\n",
    "SparkContext\n",
    "       ‚Üì\n",
    "Cluster Manager\n",
    "       ‚Üì\n",
    "Executors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 6Ô∏è‚É£ Why SparkSession Was Introduced?\n",
    "\n",
    "Before Spark 2.0, we had:\n",
    "\n",
    "- SparkContext\n",
    "- SQLContext\n",
    "- HiveContext\n",
    "\n",
    "Too many contexts.\n",
    "\n",
    "SparkSession unified everything into one object.\n",
    "\n",
    "So instead of:\n",
    "\n",
    "```python\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "```\n",
    "\n",
    "Now we just use:\n",
    "\n",
    "```python\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 7Ô∏è‚É£ When Do You Use SparkContext Directly?\n",
    "\n",
    "Rare cases:\n",
    "\n",
    "- RDD-based operations\n",
    "- Broadcast variables\n",
    "- Accumulators\n",
    "- Low-level distributed logic\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "broadcast_var = spark.sparkContext.broadcast([1,2,3])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "# 8Ô∏è‚É£ Interview-Ready Explanation\n",
    "\n",
    "If interviewer asks:\n",
    "\n",
    "### ‚ùì What is difference between SparkSession and SparkContext?\n",
    "\n",
    "Answer:\n",
    "\n",
    "> SparkContext is the core connection to the cluster and is used mainly for RDD operations. SparkSession is the unified entry point introduced in Spark 2.0 that wraps SparkContext and provides APIs for DataFrame, SQL, and Streaming.\n",
    "\n",
    "---\n",
    "\n",
    "# 9Ô∏è‚É£ Practical Rule\n",
    "\n",
    "In modern Spark:\n",
    "\n",
    "‚úÖ Always create SparkSession  \n",
    "‚ùå Do not manually create SparkContext  \n",
    "\n",
    "SparkSession will handle it internally.\n",
    "\n",
    "---\n",
    "\n",
    "# üîü Common Interview Trap\n",
    "\n",
    "Question:\n",
    "\n",
    "Can we create multiple SparkContexts?\n",
    "\n",
    "Answer:\n",
    "\n",
    "‚ùå No. Only one SparkContext per JVM.\n",
    "\n",
    "But:\n",
    "\n",
    "You can create multiple SparkSessions using the same SparkContext.\n",
    "\n",
    "---\n",
    "\n",
    "# üéØ Final Comparison\n",
    "\n",
    "| Aspect | SparkContext | SparkSession |\n",
    "|--------|--------------|--------------|\n",
    "| Level | Low-level | High-level |\n",
    "| API | RDD | DataFrame/SQL |\n",
    "| Introduced | Spark 1.x | Spark 2.x |\n",
    "| Used Today | Internally | Primary interface |\n",
    "\n",
    "---\n",
    "\n",
    "# üöÄ One-Line Memory Trick\n",
    "\n",
    "SparkContext = Engine  \n",
    "SparkSession = Dashboard + Engine\n",
    "\n",
    "---\n",
    "\n",
    "# üß† Advanced Follow-Up (If Asked)\n",
    "\n",
    "Interviewer may ask:\n",
    "\n",
    "- What happens if SparkContext crashes?\n",
    "- Can SparkSession exist without SparkContext?\n",
    "- How does SparkSession manage Hive?\n",
    "- What is getOrCreate() doing internally?\n",
    "\n",
    "We can cover these next if you want üî•\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b907156c-165d-4b8f-b132-5623ac6268b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7463a5f9-65bf-4cdf-82ba-c86ea409b98d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3c6ca9eb-186b-4252-b248-cd6c007a328e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a62226dc-f635-4e1e-99a1-0759d96efb0f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "# from pyspark.sql import SparkSession# spark = SparkSession.builder.appName(\"Spark DataFrames\").getOrCreate()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark Fundamentals\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ad39b02-89ec-4994-b01c-91f6d451e046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sparak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "baaf6053-50c5-42f8-a008-19e223527b01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f3f45cb-1425-4c69-9c33-eb60f1245c74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "02-Spark Session 2026-02-17",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
