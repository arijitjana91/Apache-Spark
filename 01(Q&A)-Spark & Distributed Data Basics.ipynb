{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77ec8c7e",
   "metadata": {},
   "source": [
    "\n",
    "# ğŸ¯ Next Smart Step (Based on Your Learning Path)\n",
    "\n",
    "Since you're preparing seriously for distributed data engineering:\n",
    "\n",
    "You should now move to:\n",
    "```\n",
    "1ï¸âƒ£ Spark architecture\n",
    "2ï¸âƒ£ RDD vs DataFrame vs Dataset\n",
    "3ï¸âƒ£ DAG vs MapReduce\n",
    "4ï¸âƒ£ Spark execution flow\n",
    "5ï¸âƒ£ Spark on YARN\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0320f80",
   "metadata": {},
   "source": [
    "# ğŸš€ Apache Spark â€“ Core Concepts for Data Engineers\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ Spark Architecture (Detailed)\n",
    "\n",
    "## ğŸ”¹ High-Level Components\n",
    "\n",
    "```\n",
    "Driver Program\n",
    "   |\n",
    "   |-- SparkSession\n",
    "   |-- SparkContext\n",
    "   |-- DAG Scheduler\n",
    "   |-- Task Scheduler\n",
    "   |\n",
    "Cluster Manager (Standalone / YARN / Kubernetes)\n",
    "   |\n",
    "Executors (Multiple JVMs across Worker Nodes)\n",
    "   |\n",
    "Tasks (Run inside Executors)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 1. Driver\n",
    "\n",
    "The Driver is the brain of the Spark application.\n",
    "\n",
    "### Responsibilities:\n",
    "\n",
    "- Creates SparkSession\n",
    "- Converts user code into DAG\n",
    "- Splits DAG into stages\n",
    "- Schedules tasks\n",
    "- Coordinates executors\n",
    "- Collects results\n",
    "\n",
    "Driver runs:\n",
    "\n",
    "- DAG Scheduler\n",
    "- Task Scheduler\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 2. Cluster Manager\n",
    "\n",
    "Allocates resources.\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Standalone\n",
    "- YARN\n",
    "- Kubernetes\n",
    "\n",
    "Cluster manager decides:\n",
    "\n",
    "- How many executors\n",
    "- Memory allocation\n",
    "- CPU cores\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 3. Executors\n",
    "\n",
    "Executors are worker JVM processes.\n",
    "\n",
    "### Responsibilities:\n",
    "\n",
    "- Execute tasks\n",
    "- Store data in memory\n",
    "- Perform shuffle operations\n",
    "- Return results to driver\n",
    "\n",
    "Each executor contains:\n",
    "\n",
    "- Task threads\n",
    "- Memory manager\n",
    "- Block manager\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ 4. Tasks\n",
    "\n",
    "- Smallest unit of execution\n",
    "- One task per partition\n",
    "- Runs inside executor\n",
    "\n",
    "**If you have 10 partitions â†’ 10 tasks.**\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ RDD vs DataFrame vs Dataset\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ RDD (Resilient Distributed Dataset)\n",
    "\n",
    "Low-level distributed collection.\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4])\n",
    "```\n",
    "\n",
    "### Characteristics:\n",
    "\n",
    "- Immutable\n",
    "- Distributed\n",
    "- Fault-tolerant\n",
    "- No schema\n",
    "- No Catalyst optimization\n",
    "\n",
    "### Pros:\n",
    "\n",
    "- Full control\n",
    "- Functional programming style\n",
    "\n",
    "### Cons:\n",
    "\n",
    "- Slow compared to DataFrame\n",
    "- No query optimization\n",
    "- More memory usage\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ DataFrame\n",
    "\n",
    "Distributed table with schema.\n",
    "\n",
    "```python\n",
    "df = spark.read.csv(\"file.csv\", header=True)\n",
    "```\n",
    "\n",
    "### Characteristics:\n",
    "\n",
    "- Structured data\n",
    "- Schema-based\n",
    "- Optimized using Catalyst\n",
    "- Tungsten execution engine\n",
    "\n",
    "### Pros:\n",
    "\n",
    "- Faster\n",
    "- Less code\n",
    "- Optimized execution\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Dataset (Scala/Java Only)\n",
    "\n",
    "Strongly-typed version of DataFrame.\n",
    "\n",
    "- Compile-time type safety\n",
    "- Combines RDD + DataFrame benefits\n",
    "\n",
    "Not supported in PySpark.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¥ Comparison Table\n",
    "\n",
    "| Feature | RDD | DataFrame | Dataset |\n",
    "|----------|------|------------|----------|\n",
    "| Level | Low | High | High |\n",
    "| Schema | âŒ No | âœ… Yes | âœ… Yes |\n",
    "| Optimization | âŒ No | âœ… Catalyst | âœ… Catalyst |\n",
    "| Type Safety | âŒ | âŒ (Python) | âœ… (Scala) |\n",
    "| Recommended | âŒ | âœ… | âœ… |\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ DAG vs MapReduce\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ What is DAG?\n",
    "\n",
    "DAG = Directed Acyclic Graph\n",
    "\n",
    "Spark builds a DAG of transformations before execution.\n",
    "\n",
    "Example:\n",
    "\n",
    "```python\n",
    "df.filter().groupBy().count()\n",
    "```\n",
    "\n",
    "Spark creates logical plan â†’ optimized plan â†’ physical plan.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ MapReduce (Hadoop)\n",
    "\n",
    "MapReduce works in strict stages:\n",
    "\n",
    "```\n",
    "Map â†’ Shuffle â†’ Reduce\n",
    "```\n",
    "\n",
    "Every job writes to disk between stages.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¥ DAG vs MapReduce\n",
    "\n",
    "| Feature | MapReduce | Spark DAG |\n",
    "|----------|-------------|------------|\n",
    "| Execution Model | Fixed stages | Flexible DAG |\n",
    "| Disk Usage | Heavy | In-memory |\n",
    "| Speed | Slow | Fast |\n",
    "| Optimization | Limited | Catalyst |\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ Spark Execution Flow (Step-by-Step)\n",
    "\n",
    "---\n",
    "\n",
    "## Step 1: User Writes Code\n",
    "\n",
    "```python\n",
    "df.groupBy(\"id\").count()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Step 2: Logical Plan Created\n",
    "\n",
    "Spark builds logical plan.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 3: Catalyst Optimizer\n",
    "\n",
    "- Predicate pushdown\n",
    "- Column pruning\n",
    "- Join reordering\n",
    "\n",
    "---\n",
    "\n",
    "## Step 4: Physical Plan Created\n",
    "\n",
    "Execution strategy selected.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 5: DAG Created\n",
    "\n",
    "Transformations split into stages.\n",
    "\n",
    "---\n",
    "\n",
    "## Step 6: Task Scheduling\n",
    "\n",
    "- Stage â†’ Tasks\n",
    "- One task per partition\n",
    "\n",
    "---\n",
    "\n",
    "## Step 7: Executors Execute Tasks\n",
    "\n",
    "- Perform computation\n",
    "- Shuffle if needed\n",
    "- Store intermediate results\n",
    "\n",
    "---\n",
    "\n",
    "## Step 8: Result Sent to Driver\n",
    "\n",
    "Driver collects result.\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ Spark on YARN\n",
    "\n",
    "YARN = Yet Another Resource Negotiator (Hadoop resource manager)\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Architecture on YARN\n",
    "\n",
    "```\n",
    "Client\n",
    "   |\n",
    "YARN ResourceManager\n",
    "   |\n",
    "ApplicationMaster\n",
    "   |\n",
    "Executors (Containers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Execution Modes\n",
    "\n",
    "### 1. Client Mode\n",
    "\n",
    "- Driver runs on client machine\n",
    "- Executors run on YARN cluster\n",
    "\n",
    "### 2. Cluster Mode\n",
    "\n",
    "- Driver runs inside YARN container\n",
    "- Fully distributed\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Flow in YARN Cluster Mode\n",
    "\n",
    "1. User submits job\n",
    "2. YARN allocates container\n",
    "3. ApplicationMaster starts\n",
    "4. Driver initializes\n",
    "5. Executors launched\n",
    "6. Tasks executed\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¥ Why Spark on YARN?\n",
    "\n",
    "- Multi-tenant cluster\n",
    "- Resource sharing\n",
    "- Fault tolerance\n",
    "- Dynamic scaling\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš€ Final Interview Summary\n",
    "\n",
    "- Driver = Brain\n",
    "- Executors = Workers\n",
    "- DAG = Execution plan\n",
    "- RDD = Low-level API\n",
    "- DataFrame = Optimized structured API\n",
    "- Dataset = Typed API (Scala)\n",
    "- Shuffle = Expensive operation\n",
    "- SparkSession = Unified entry point\n",
    "- YARN = Resource manager\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ What To Master for Interviews\n",
    "\n",
    "- Shuffle internals\n",
    "- Partitioning strategy\n",
    "- Catalyst optimizer\n",
    "- Narrow vs Wide transformations\n",
    "- Spark memory management\n",
    "- Broadcast joins\n",
    "- Skew handling\n",
    "- Executor tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e731008a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0bed96ad",
   "metadata": {},
   "source": [
    "# ğŸ”¥ **RDD Core Properties â€“ Explained with Examples**\n",
    "\n",
    "RDD = Resilient Distributed Dataset\n",
    "\n",
    "---\n",
    "\n",
    "# 1ï¸âƒ£ Immutable\n",
    "\n",
    "## ğŸ”¹ What Does Immutable Mean?\n",
    "\n",
    "Once an RDD is created, it **cannot be modified**.\n",
    "\n",
    "Every transformation creates a **new RDD**.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Example\n",
    "\n",
    "```python\n",
    "rdd1 = spark.sparkContext.parallelize([1, 2, 3, 4])\n",
    "\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "\n",
    "print(rdd1.collect())  # [1, 2, 3, 4]\n",
    "print(rdd2.collect())  # [2, 4, 6, 8]\n",
    "```\n",
    "\n",
    "### Explanation:\n",
    "\n",
    "- `rdd1` remains unchanged.\n",
    "- `map()` created a new RDD (`rdd2`).\n",
    "- Original RDD is never modified.\n",
    "\n",
    "---\n",
    "\n",
    "# 2ï¸âƒ£ Distributed\n",
    "\n",
    "## ğŸ”¹ What Does Distributed Mean?\n",
    "\n",
    "Data is split into **multiple partitions**  \n",
    "Each partition is processed in parallel on different executors.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Example\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5,6,7,8], 4)\n",
    "\n",
    "print(rdd.getNumPartitions())\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "4\n",
    "```\n",
    "\n",
    "### What This Means:\n",
    "\n",
    "- Data is divided into 4 partitions.\n",
    "- Each partition can run on different executors.\n",
    "- Each partition â†’ 1 task.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Visual Representation\n",
    "\n",
    "```\n",
    "RDD\n",
    " â”œâ”€â”€ Partition 1 â†’ Executor 1\n",
    " â”œâ”€â”€ Partition 2 â†’ Executor 2\n",
    " â”œâ”€â”€ Partition 3 â†’ Executor 3\n",
    " â””â”€â”€ Partition 4 â†’ Executor 4\n",
    "```\n",
    "\n",
    "This is why Spark is fast â€” parallel execution.\n",
    "\n",
    "---\n",
    "\n",
    "# 3ï¸âƒ£ Fault-Tolerant\n",
    "\n",
    "## ğŸ”¹ What Does Fault-Tolerant Mean?\n",
    "\n",
    "If a partition is lost (executor crash), Spark can recompute it.\n",
    "\n",
    "How?\n",
    "\n",
    "ğŸ‘‰ Using **Lineage (DAG of transformations)**\n",
    "\n",
    "Spark does NOT replicate all data.  \n",
    "It remembers how the RDD was created.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Example\n",
    "\n",
    "```python\n",
    "rdd1 = spark.sparkContext.parallelize([1,2,3,4])\n",
    "rdd2 = rdd1.map(lambda x: x * 2)\n",
    "rdd3 = rdd2.filter(lambda x: x > 4)\n",
    "\n",
    "print(rdd3.toDebugString())\n",
    "```\n",
    "\n",
    "Output shows lineage:\n",
    "\n",
    "```\n",
    "RDD3\n",
    "  |\n",
    "RDD2 (map)\n",
    "  |\n",
    "RDD1 (parallelize)\n",
    "```\n",
    "\n",
    "If partition of `rdd3` is lost:\n",
    "\n",
    "Spark recomputes:\n",
    "\n",
    "```\n",
    "parallelize â†’ map â†’ filter\n",
    "```\n",
    "\n",
    "That is fault tolerance.\n",
    "\n",
    "---\n",
    "\n",
    "# 4ï¸âƒ£ No Schema\n",
    "\n",
    "## ğŸ”¹ What Does No Schema Mean?\n",
    "\n",
    "RDD does NOT enforce column names or data types.\n",
    "\n",
    "It is just a collection of objects.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Example\n",
    "\n",
    "```python\n",
    "rdd = spark.sparkContext.parallelize([\n",
    "    (1, \"Alice\"),\n",
    "    (2, \"Bob\")\n",
    "])\n",
    "\n",
    "print(rdd.collect())\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "[(1, 'Alice'), (2, 'Bob')]\n",
    "```\n",
    "\n",
    "Spark does NOT know:\n",
    "\n",
    "- First column is \"id\"\n",
    "- Second column is \"name\"\n",
    "\n",
    "No schema metadata.\n",
    "\n",
    "Compare with DataFrame:\n",
    "\n",
    "```python\n",
    "df = spark.createDataFrame(rdd, [\"id\", \"name\"])\n",
    "df.printSchema()\n",
    "```\n",
    "\n",
    "Output:\n",
    "\n",
    "```\n",
    "root\n",
    " |-- id: long\n",
    " |-- name: string\n",
    "```\n",
    "\n",
    "DataFrame has schema.  \n",
    "RDD does NOT.\n",
    "\n",
    "---\n",
    "\n",
    "# 5ï¸âƒ£ No Catalyst Optimization\n",
    "\n",
    "## ğŸ”¹ What is Catalyst?\n",
    "\n",
    "Catalyst is Sparkâ€™s **query optimizer engine**.\n",
    "\n",
    "It optimizes:\n",
    "\n",
    "- Filter pushdown\n",
    "- Column pruning\n",
    "- Join reordering\n",
    "- Constant folding\n",
    "- Predicate simplification\n",
    "\n",
    "DataFrames use Catalyst.\n",
    "\n",
    "RDD does NOT.\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ”¹ Example\n",
    "\n",
    "### RDD (No Optimization)\n",
    "\n",
    "```python\n",
    "rdd.filter(lambda x: x > 10).map(lambda x: x * 2)\n",
    "```\n",
    "\n",
    "Spark executes exactly as written.\n",
    "\n",
    "No optimization.\n",
    "\n",
    "---\n",
    "\n",
    "### DataFrame (Optimized)\n",
    "\n",
    "```python\n",
    "df.filter(\"age > 10\").select(\"name\")\n",
    "```\n",
    "\n",
    "Catalyst will:\n",
    "\n",
    "- Push filter early\n",
    "- Remove unused columns\n",
    "- Optimize execution plan\n",
    "\n",
    "You can see it:\n",
    "\n",
    "```python\n",
    "df.explain(True)\n",
    "```\n",
    "\n",
    "RDD does NOT support `.explain()`.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ”¥ Interview-Level Comparison\n",
    "\n",
    "| Feature | RDD | DataFrame |\n",
    "|----------|------|------------|\n",
    "| Immutable | âœ… | âœ… |\n",
    "| Distributed | âœ… | âœ… |\n",
    "| Fault Tolerance | Lineage | Lineage |\n",
    "| Schema | âŒ No | âœ… Yes |\n",
    "| Optimization | âŒ None | âœ… Catalyst |\n",
    "| Performance | Slower | Faster |\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸš€ Final Interview Answer\n",
    "\n",
    "RDD is:\n",
    "\n",
    "- Immutable\n",
    "- Distributed across partitions\n",
    "- Fault-tolerant via lineage\n",
    "- Schema-less\n",
    "- Not optimized by Catalyst\n",
    "\n",
    "DataFrames are preferred in modern Spark because they are optimized and faster.\n",
    "\n",
    "---\n",
    "\n",
    "# ğŸ¯ Key Concept to Remember\n",
    "\n",
    "RDD = Low-level API  \n",
    "DataFrame = Optimized Structured API  \n",
    "Catalyst = Spark SQL Optimization Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746a4f34",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
